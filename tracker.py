import cv2
import mediapipe as mp
import numpy as np
import time
import threading
from Const import *

#pose_landmark_index = [1,152,33,263,61,291]
pose_landmark_index = [1, 151,101,330,345,116,103,332,156,383, 
                       195,168,322,165,69,299]

def load_personal_model(json_path):
    """
    å¾ JSON è¼‰å…¥å€‹äººåŒ–è‡‰æ¨¡ï¼Œä¸¦æå–æŒ‡å®šçš„å‰›æ€§ç‰¹å¾µé»ã€‚
    
    Args:
        json_path (str): å‰›å‰›å­˜ä¸‹ä¾†çš„ json æª”æ¡ˆè·¯å¾‘
        target_indices (dict or list): ä½ éœ€è¦çš„å‰›æ€§é» Index (ä¾‹å¦‚ { 'NOSE': 1, ... })
        
    Returns:
        np.array: ç»™ solvePnP ç”¨çš„ model_points (N, 3)
    """
    try:
        print(f"ğŸ“‚ Loading personal model from {json_path}...")
        with open(json_path, 'r') as f:
            all_landmarks = json.load(f)
            
        selected_points = []

        for pt in all_landmarks:
            selected_points.append(pt)
        
        # è½‰æˆ NumPy Float64 (solvePnP å¿…è¦æ ¼å¼)
        model_points_np = np.array(selected_points, dtype=np.float64)
        
        print(f"âœ… Loaded {len(model_points_np)} rigid points successfully.")
        return model_points_np

    except FileNotFoundError:
        print(f"âŒ Error: File {json_path} not found.")
        return None
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None

my_face = load_personal_model("assets/privacy/my_personal_landmarks.json")

import numpy as np

def project_perspective_auto_fit(points_3d, win_w, win_h, fov_degrees=60.0, padding=0.1):
    """
    é€è¦–æŠ•å½± + è‡ªå‹•ç¸®æ”¾ (Auto-Dolly)
    
    1. å°‡ 3D é»é›²ç§»åˆ°åŸé» (0,0,0)
    2. æ ¹æ“š FOV è¨ˆç®—è™›æ“¬ç„¦è·
    3. è¨ˆç®—éœ€è¦å°‡ç‰©é«”æ¨é å¤šå°‘è·é›¢ (tz)ï¼Œæ‰èƒ½å‰›å¥½å¡«æ»¿è¦–çª—
    4. åŸ·è¡Œé€è¦–æŠ•å½±: u = fx * (x / z) + cx
    
    Args:
        points_3d (np.array): (N, 3) 3D é»
        win_w, win_h (int): è¦–çª—å¤§å°
        fov_degrees (float): è™›æ“¬ç›¸æ©Ÿçš„è¦–é‡è§’åº¦ (é è¨­ 60 åº¦ï¼Œæ¥è¿‘äººçœ¼/Webcam)
        padding (float): é‚Šç·£ç•™ç™½æ¯”ä¾‹ (0.1 = 10%)
        
    Returns:
        np.array: (N, 2) æ•´æ•¸åº§æ¨™ (u, v)
    """
    # 1. è¤‡è£½è³‡æ–™ä»¥å…æ”¹åˆ°åŸå§‹æ•¸æ“š
    p3d = points_3d.copy()
    
    # 2. å¹¾ä½•ä¸­å¿ƒæ­¸é›¶ (Centering)
    # æ‰¾å‡ºç‰©é«”ä¸­å¿ƒï¼Œä¸¦å°‡æ‰€æœ‰é»ç§»åˆ°ä»¥ (0,0,0) ç‚ºä¸­å¿ƒ
    center = np.mean(p3d, axis=0)
    p3d_centered = p3d - center
    
    # 3. è¨ˆç®— 3D ç©ºé–“ä¸­çš„åŒ…åœç›’å°ºå¯¸ (Bounding Box Size)
    min_xyz = np.min(p3d_centered, axis=0)
    max_xyz = np.max(p3d_centered, axis=0)
    face_w_3d = max_xyz[0] - min_xyz[0]
    face_h_3d = max_xyz[1] - min_xyz[1]
    
    # 4. å»ºç«‹è™›æ“¬ç›¸æ©Ÿåƒæ•¸ (Intrinsics)
    # fx = (W / 2) / tan(fov / 2)
    fov_rad = np.radians(fov_degrees)
    focal_length = (win_w / 2) / np.tan(fov_rad / 2)
    
    cx = win_w / 2
    cy = win_h / 2
    
    # 5. è¨ˆç®—ã€Œæ¨è»Œè·é›¢ã€ (Dolly Distance / Z-Offset)
    # æˆ‘å€‘éœ€è¦å¤šæ·±çš„ Zï¼Œæ‰èƒ½è®“æŠ•å½±å¾Œçš„å¯¬åº¦ç­‰æ–¼è¦–çª—å¯¬åº¦ï¼Ÿ
    # å…¬å¼å°å‡º: projected_size = focal_length * (real_size / depth)
    # æ‰€ä»¥: depth = focal_length * real_size / projected_size
    
    target_w = win_w * (1.0 - padding * 2)
    target_h = win_h * (1.0 - padding * 2)
    
    # é¿å…é™¤ä»¥é›¶
    face_w_3d = max(face_w_3d, 1e-5)
    face_h_3d = max(face_h_3d, 1e-5)

    dist_w = focal_length * (face_w_3d / target_w)
    dist_h = focal_length * (face_h_3d / target_h)
    
    # å–æœ€é çš„è·é›¢ï¼Œç¢ºä¿å¯¬å’Œé«˜éƒ½å¡å¾—é€²å»
    z_offset = max(dist_w, dist_h)
    
    # ç¨å¾®åŠ ä¸€é»è¿‘å¹³é¢å‰ªè£ä¿è­· (Near Plane Clipping protection)
    # è®“è‡‰çš„æœ€å‡¸é»ä¸æœƒæˆ³åˆ°é¡é ­å¾Œé¢
    max_z_variation = np.max(np.abs(p3d_centered[:, 2]))
    z_dist = z_offset + max_z_variation + 10 # å®‰å…¨è·é›¢

    # 6. åŸ·è¡Œé€è¦–æŠ•å½± (Perspective Projection)
    # u = fx * x / z + cx
    # v = fy * y / z + cy
    
    # åŠ ä¸Šæ·±åº¦æ¨ç§»
    z_coords = p3d_centered[:, 2] + z_dist
    
    # æŠ•å½±é‹ç®—
    # æ³¨æ„: å¦‚æœä½ çš„ Y è»¸æ–¹å‘è·Ÿè¢å¹•ç›¸åï¼Œå¯ä»¥åœ¨é€™è£¡åŠ è² è™Ÿ (-p3d_centered[:, 1])
    # é€™è£¡å‡è¨­ Y å‘ä¸‹ç‚ºæ­£ (OpenCV æ¨™æº–)
    u = (focal_length * p3d_centered[:, 0] / z_coords) + cx
    v = (focal_length * p3d_centered[:, 1] / z_coords) + cy
    
    return np.stack((u, v), axis=1).astype(np.int32)
import numpy as np

def fit_points_to_window(uv_points, window_w, window_h, padding_ratio=0.1):
    """
    æ¥æ”¶ä»»æ„ä¸€çµ„ UV é» (N, 2)ï¼Œè‡ªå‹•ç¸®æ”¾å¹³ç§»ä»¥å¡«æ»¿è¦–çª—ã€‚
    
    Args:
        uv_points (np.array): å½¢ç‹€ (N, 2) çš„é»ï¼Œå–®ä½ä¸é™ (å¯ä»¥æ˜¯ 0~1 æˆ–ä»»æ„ pixel)
        window_w (int): ç›®æ¨™è¦–çª—å¯¬åº¦
        window_h (int): ç›®æ¨™è¦–çª—é«˜åº¦
        padding_ratio (float): é‚Šç·£ç•™ç™½æ¯”ä¾‹ (ä¾‹å¦‚ 0.1 ä»£è¡¨ç•™ 10% ç©ºéš™)
        
    Returns:
        np.array: è½‰æ›å¾Œçš„ (N, 2) æ•´æ•¸åº§æ¨™ï¼Œå¯ç›´æ¥ç•«åœ–
    """
    points = np.array(uv_points)
    
    # 1. æ‰¾å‡ºç›®å‰çš„é‚Šç•Œ (Bounding Box)
    min_x, min_y = np.min(points, axis=0)
    max_x, max_y = np.max(points, axis=0)
    
    current_w = max_x - min_x
    current_h = max_y - min_y
    current_center_x = (min_x + max_x) / 2
    current_center_y = (min_y + max_y) / 2
    
    # 2. é¿å…é™¤ä»¥é›¶ (å¦‚æœåªæœ‰ä¸€å€‹é»æˆ–é»é‡ç–Š)
    current_w = max(current_w, 1e-5)
    current_h = max(current_h, 1e-5)

    # 3. è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹ (Scale to Fit)
    # æˆ‘å€‘å¸Œæœ›å¯¬åº¦èƒ½æ’æ»¿è¦–çª—ï¼Œé«˜åº¦ä¹Ÿèƒ½æ’æ»¿è¦–çª—
    # ä½†ç‚ºäº†ä¸è®Šå½¢ï¼Œæˆ‘å€‘å– "å…©è€…ä¸­è¼ƒå°" çš„ç¸®æ”¾å€ç‡
    scale_x = window_w / current_w
    scale_y = window_h / current_h
    
    # å¯¦éš›ç¸®æ”¾å€ç‡ (ä¹˜ä¸Š (1 - padding*2) æ˜¯ç‚ºäº†ç•™å‡ºé›™é‚Šçš„ç™½é‚Š)
    final_scale = min(scale_x, scale_y) * (1.0 - padding_ratio * 2)

    # 4. åŸ·è¡Œè®Šæ›å…¬å¼
    # New = (Old - Old_Center) * Scale + New_Window_Center
    
    target_center_x = window_w // 2
    target_center_y = window_h // 2
    
    new_xs = (points[:, 0] - current_center_x) * final_scale + target_center_x
    new_ys = (points[:, 1] - current_center_y) * final_scale + target_center_y
    
    # 5. çµ„åˆä¸¦è½‰æ•´æ•¸
    new_points = np.stack((new_xs, new_ys), axis=1).astype(np.int32)
    
    return new_points
import cv2
import numpy as np

# 1. 3D å‰›é«”è®Šæ›å‡½å¼ (å¯¦ä½œ Tx')
def apply_rigid_transform(points_3d, rvec, tvec):
    """
    å°‡æ¨¡å‹é» (N,3) å¥—ç”¨æ—‹è½‰èˆ‡ä½ç§»ï¼Œè½‰æ›åˆ°ç›¸æ©Ÿåº§æ¨™ç³»ã€‚
    P_cam = P_model * R^T + t^T
    """
    # å°‡æ—‹è½‰å‘é‡è½‰ç‚ºçŸ©é™£
    R, _ = cv2.Rodrigues(rvec)
    
    # åŸ·è¡Œè®Šæ› (æ³¨æ„ numpy çš„å»£æ’­æ©Ÿåˆ¶å’Œç¶­åº¦)
    # points_3d æ˜¯ (N, 3)ï¼ŒR æ˜¯ (3, 3)ï¼Œtvec æ˜¯ (3, 1)
    # æˆ‘å€‘ç”¨è¡Œå„ªå…ˆçš„æ–¹å¼å¯«: points @ R.T + tvec.T
    transformed_points = points_3d @ R.T + tvec.reshape(1, 3)
    
    return transformed_points

def apply_inverse_rigid_transform(points_cam, rvec, tvec):
    """
    é€†å‘è®Šæ›ï¼šå°‡ç›¸æ©Ÿåº§æ¨™ç³»ä¸‹çš„é» (MediaPipe) æ¨å› æ¨¡å‹åº§æ¨™ç³» (Local Space)ã€‚
    
    æ•¸å­¸å…¬å¼: P_model = (P_cam - t) * R
    (æ³¨æ„: é€™è£¡æ˜¯ç”¨ Row-vector çš„å¯«æ³•ï¼Œæ‰€ä»¥ä¹˜çš„æ˜¯ R è€Œä¸æ˜¯ R.T)
    
    Args:
        points_cam (np.array): (N, 3) å¯¦éš›æŠ“åˆ°çš„é» (Pixel/Metric space)
        rvec (np.array): PnP ç®—å‡ºçš„æ—‹è½‰å‘é‡
        tvec (np.array): PnP ç®—å‡ºçš„ä½ç§»å‘é‡
        
    Returns:
        np.array: (N, 3) è½‰å›æ­£è‡‰ç©ºé–“çš„é»
    """
    # 1. å–å¾—æ—‹è½‰çŸ©é™£ R
    R, _ = cv2.Rodrigues(rvec)
    
    # 2. è™•ç†ä½ç§» t
    # ç¢ºä¿ tvec å½¢ç‹€æ˜¯ (1, 3) ä»¥ä¾¿å»£æ’­
    t_row = tvec.reshape(1, 3)
    
    # 3. åŸ·è¡Œé€†å‘è®Šæ›
    # å…ˆæ¸›å»ä½ç§»
    points_centered = points_cam - t_row
    
    # å†ä¹˜ä¸Š R (ç‚ºä»€éº¼æ˜¯ R ä¸æ˜¯ R.Tï¼Ÿ)
    # å‰å‘: P_cam = P_model @ R.T + t
    # ç§»é …: P_cam - t = P_model @ R.T
    # å…©é‚ŠåŒä¹˜ (R.T)^-1ï¼Œä¹Ÿå°±æ˜¯ R
    # (P_cam - t) @ R = P_model
    points_model = points_centered @ R
    
    return points_model

# 3. å°ˆé–€çš„ Debug ç¹ªåœ–å‡½å¼
def draw_debug_comparison(predicted_3d, actual_3d, win_size=600):
    canvas = np.zeros((win_size, win_size, 3), dtype=np.uint8)
    
    # === ğŸ†• æ–°å¢ï¼šå¼·åˆ¶ä¸­å¿ƒå°é½Š (Centroid Alignment) ===
    # é€™æ˜¯ç‚ºäº†æ¶ˆé™¤ tvec (ä½ç§») çš„åº§æ¨™ç³»å®šç¾©èª¤å·®ï¼Œå°ˆæ³¨æª¢æŸ¥ rvec (æ—‹è½‰)
    
    # 1. ç®—å‡ºè—è‰²é»ç¾¤çš„ä¸­å¿ƒ
    pred_center = np.mean(predicted_3d, axis=0)
    # 2. ç®—å‡ºç´…è‰²é»ç¾¤çš„ä¸­å¿ƒ
    act_center = np.mean(actual_3d, axis=0)
    
    # 3. æŠŠè—è‰²é»ç¾¤ã€Œæ¬ã€åˆ°ç´…è‰²é»ç¾¤çš„ä½ç½®
    #    (æ‰€æœ‰è—é» - è—ä¸­å¿ƒ + ç´…ä¸­å¿ƒ)
    shift_vector = act_center - pred_center
    aligned_predicted_3d = predicted_3d #+ shift_vector
    
    # =================================================
    
    # æ¥ä¸‹ä¾†ç”¨ "å°é½Šå¾Œ" çš„é»ä¾†åšæŠ•å½±èˆ‡ç¹ªåœ–
    # åˆä½µå…©çµ„é»
    combined_3d = np.vstack((aligned_predicted_3d, actual_3d))
    
    # æŠ•å½± (ä½¿ç”¨ä¹‹å‰å¯«å¥½çš„å‡½å¼)
    combined_uv = project_perspective_auto_fit(combined_3d, win_size, win_size, fov_degrees=60.0, padding=0.2)
    combined_uv = fit_points_to_window(combined_uv,win_size,win_size)
    
    n_points = len(predicted_3d)
    uv_pred = combined_uv[:n_points]
    uv_act = combined_uv[n_points:]
    
    # ç¹ªåœ–
    for i in range(n_points):
        pt_pred = tuple(uv_pred[i])
        pt_act = tuple(uv_act[i])
        
        # é»ƒç·š (Error)
        cv2.line(canvas, pt_pred, pt_act, (0, 255, 255), 1)
        # è—é» (Predicted - Aligned)
        cv2.circle(canvas, pt_pred, 5, (255, 255, 0), -1)
        # ç´…é» (Actual)
        cv2.circle(canvas, pt_act, 5, (0, 0, 255), -1)

    # é¡¯ç¤ºèª¤å·®æ•¸æ“š
    error_dist = np.linalg.norm(aligned_predicted_3d - actual_3d, axis=1)
    avg_error = np.mean(error_dist)
    cv2.putText(canvas, f"Avg Shape Error: {avg_error:.2f}", (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    return canvas

class FaceTracker:
    def __init__(self):
        self.mp_face_mesh = mp.solutions.face_mesh
        # refine_landmarks=True æ˜¯é—œéµï¼Œé€™æ¨£æ‰æœƒå›å‚³ç³å­”(Iris)çš„åº§æ¨™
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.cap = cv2.VideoCapture(0)
        
        # --- [æ–°å¢] Head Pose Estimation éœ€è¦çš„åƒæ•¸ ---
        self.img_w = 640 # é è¨­ï¼Œä¹‹å¾Œæœƒå‹•æ…‹æ›´æ–°
        self.img_h = 480
        
        # å®šç¾©æ¨™æº– 3D è‡‰éƒ¨æ¨¡å‹çš„ 6 å€‹ç‰¹å¾µé» (ä¸–ç•Œåº§æ¨™)
        # é †åºï¼šé¼»å°–, ä¸‹å·´, å·¦çœ¼è§’, å³çœ¼è§’, å·¦å˜´è§’, å³å˜´è§’
        points = [(my_face[i][0],my_face[i][1],my_face[i][2]) for i in pose_landmark_index]
        self.model_points = np.array(points,dtype=np.float64)
        #self.model_points = np.array([
        #    (0.0, 0.0, 0.0),             # Nose tip (åŸé»)
        #    (0.0, 330.0, 65.0),          # Chin (ä¸‹å·´åœ¨ä¸‹é¢ -> Yæ˜¯æ­£çš„)
        #    (-225.0, -170.0, 135.0),     # Left eye left corner (çœ¼ç›åœ¨ä¸Šé¢ -> Yæ˜¯è² çš„)
        #    (225.0, -170.0, 135.0),      # Right eye right corner
        #    (-150.0, 150.0, 125.0),      # Left Mouth corner (å˜´å·´åœ¨ä¸‹é¢ -> Yæ˜¯æ­£çš„)
        #    (150.0, 150.0, 125.0),        # Right Mouth corner

        #    ## --- [æ–°å¢] ç©©å®šéŒ¨é» ---
        #    #(-60.0, -170.0, 100.0),      # 7. Left Eye Inner Corner (å…§çœ¼è§’) - é é¼»æ¨‘
        #    #(60.0, -170.0, 100.0),       # 8. Right Eye Inner Corner (å…§çœ¼è§’) - é é¼»æ¨‘

        #    ## --- [ä¿®æ­£] 9, 10 çœ‰å³°/é¡é ­ (ä½ç½®è¼ƒé«˜ï¼Œä¸”ç¨å¾®æ·±ä¸€é») ---
        #    ## Y è¨­ç‚º 300.0 (æ¯”çœ¼ç› 170 é«˜)
        #    ## X è¨­ç‚º 180.0 (å¯¬åº¦é©ä¸­)
        #    #(-180.0, -300.0, 100.0),     # 9. å·¦çœ‰å³° (å°æ‡‰ 105)
        #    #(180.0, -300.0, 100.0)       # 10. å³çœ‰å³° (å°æ‡‰ 334)
        #], dtype=np.float64)
        
        # ç›¸æ©ŸçŸ©é™£ (ä¹‹å¾Œåœ¨ process è£¡åˆå§‹åŒ–ä¸€æ¬¡å³å¯)
        self.cam_matrix = None
        self.dist_coeffs = np.zeros((4, 1)) # å‡è¨­ç„¡é¡é ­è®Šå½¢.VideoCapture(0)
        self.first = True
    def isFake(self):
        return False
    def process(self):
        success, image = self.cap.read()
        image.flags.writeable = False
        cv2.imshow("tracking result", image)
        cv2.waitKey(1)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        self.results = self.face_mesh.process(image)
        self.image = image
        return image
    def get_iris_pos(self):
        """
        å›å‚³çœ¼çƒçš„ç›¸å°ä½ç½® (x, y)
        x: -1.0 (å·¦) ~ 1.0 (å³), 0.0 æ˜¯ä¸­é–“
        y: -1.0 (ä¸Š) ~ 1.0 (ä¸‹), 0.0 æ˜¯ä¸­é–“
        """
        results = self.results
        dx, dy = 0, 0
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark
            
            # --- æ ¸å¿ƒæ¼”ç®—æ³•ï¼šè¨ˆç®—ç³å­”åœ¨çœ¼æ¡†ä¸­çš„ç›¸å°ä½ç½® ---
            # æˆ‘å€‘ä½¿ç”¨å·¦çœ¼ä¾†åšåŸºæº– (MediaPipe çš„å·¦çœ¼å°æ‡‰åˆ°ç•«é¢å³é‚Šçš„è‡‰)
            # ç´¢å¼•: 33(çœ¼é ­), 133(çœ¼å°¾), 468(ç³å­”ä¸­å¿ƒ)
            
            # å–å¾—åº§æ¨™ (åªè¦ x, y)
            in_x, in_y = landmarks[33].x, landmarks[33].y   # Inner Corner
            out_x, out_y = landmarks[133].x, landmarks[133].y # Outer Corner
            iris_x, iris_y = landmarks[468].x, landmarks[468].y # Iris Center

            # 1. è¨ˆç®—çœ¼å¯¬å’Œçœ¼é«˜ (å¤§ç•¥ä¼°ç®—)
            eye_width = out_x - in_x
            # ç‚ºäº†ç°¡åŒ–ï¼Œé«˜åº¦æˆ‘å€‘å…ˆç”¨çœ¼å¯¬çš„ä¸€å€‹æ¯”ä¾‹ä¾†æŠ“ï¼Œæˆ–è€…æš«æ™‚å¿½ç•¥ Y è»¸ç²¾ç¢ºåº¦
            
            # 2. è¨ˆç®—ç³å­”ç›¸å°æ–¼çœ¼é ­çš„è·é›¢
            dist_x = iris_x - in_x
            
            # 3. ç®—å‡ºæ¯”ä¾‹ (0.0 ~ 1.0)
            # 0.5 ä»£è¡¨åœ¨ä¸­é–“ã€‚ä¸€èˆ¬ä¾†èªªç³å­”ä¸æœƒç¢°åˆ°çœ¼è§’ï¼Œæ‰€ä»¥ç¯„åœå¤§æ¦‚åœ¨ 0.3~0.7 ä¹‹é–“
            ratio_x = dist_x / eye_width

            # 4. æ­£è¦åŒ–åˆ° -1 ~ 1 (ç‚ºäº†çµ¦ Pygame ç”¨)
            # æˆ‘å€‘å‡è¨­ 0.5 æ˜¯ä¸­å¿ƒé»ã€‚
            # (ratio - 0.5) * 2 -> è®Šæˆ -1 ~ 1
            # ä¹˜ä¸Šä¸€å€‹æ•æ„Ÿåº¦ä¿‚æ•¸ (Sensitivity)ï¼Œè®“çœ¼ç›å‹•å¾—æ›´éˆæ•
            SENSITIVITY = 2.5 
            dx = (ratio_x - 0.45) * 2 * SENSITIVITY # 0.45 æ˜¯ç¨å¾®ä¿®æ­£ä¸­å¿ƒåç§»
            
            # Y è»¸ç°¡å–®è™•ç† (çœ¼çƒä¸Šä¸‹)
            # ä½¿ç”¨çœ¼çš®ä¸Šä¸‹é»ï¼š159 (ä¸Š), 145 (ä¸‹)
            top_y = landmarks[159].y
            bot_y = landmarks[145].y
            eye_height = bot_y - top_y
            dist_y = iris_y - top_y
            ratio_y = dist_y / eye_height
            dy = (ratio_y - 0.45) * 2 * SENSITIVITY

            # é™åˆ¶æ•¸å€¼åœ¨ -1 ~ 1 ä¹‹é–“ (Clamping)
            dx = -max(-1.0, min(1.0, dx))
            dy = -max(-1.0, min(1.0, dy))

        return dx, dy

    def get_eye_blink_ratio(self):
        """
        è¨ˆç®—å·¦å³çœ¼çš„é–‹é—”ç¨‹åº¦ (Blink Ratio)
        å›å‚³: (left_ratio, right_ratio)
        æ•¸å€¼é€šå¸¸åœ¨ 0.0 (é–‰) ~ 0.3 (å¤§é–‹) ä¹‹é–“
        """
        results = self.results
        left_ratio = 1.0
        right_ratio = 1.0

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark

            # --- å·¦çœ¼é—œéµé» (MediaPipe çš„å·¦çœ¼å°æ‡‰ç•«é¢å³å´) ---
            # å‚ç›´: 159 (ä¸Š), 145 (ä¸‹)
            # æ°´å¹³: 33 (å…§), 133 (å¤–)
            l_top = landmarks[159].y
            l_bot = landmarks[145].y
            l_in  = landmarks[33].x
            l_out = landmarks[133].x
            
            # è¨ˆç®—å‚ç›´è·é›¢ / æ°´å¹³è·é›¢ (æ¨™æº–åŒ–ï¼Œé¿å…é›¢é¡é ­é è¿‘å½±éŸ¿æ•¸å€¼)
            # åŠ ä¸Šä¸€å€‹æ¥µå°çš„æ•¸ 1e-6 é¿å…é™¤ä»¥é›¶
            left_ratio = abs(l_bot - l_top) / (abs(l_out - l_in) + 1e-6)

            # --- å³çœ¼é—œéµé» ---
            # å‚ç›´: 386 (ä¸Š), 374 (ä¸‹)
            # æ°´å¹³: 362 (å…§), 263 (å¤–)
            r_top = landmarks[386].y
            r_bot = landmarks[374].y
            r_in  = landmarks[362].x
            r_out = landmarks[263].x
            
            right_ratio = abs(r_bot - r_top) / (abs(r_out - r_in) + 1e-6)

        return left_ratio, right_ratio
    def get_head_pose(self, img_w, img_h):
        """
        è¨ˆç®—é ­éƒ¨å§¿æ…‹ (Yaw, Pitch, Roll)
        å›å‚³: yaw, pitch, roll (å–®ä½: åº¦ degree)
        """
        results = self.results
        if results.multi_face_landmarks:
            face_landmarks = results.multi_face_landmarks[0]
        else:
            return (0,0,0)
        self.img_w = img_w
        self.img_h = img_h

        # å¦‚æœé‚„æ²’è¨­å®šç›¸æ©ŸçŸ©é™£ï¼Œè¨­ä¸€å€‹ä¼°è¨ˆå€¼
        if self.cam_matrix is None:
            focal_length = img_w
            center = (img_w / 2, img_h / 2)
            self.cam_matrix = np.array(
                [[focal_length, 0, center[0]],
                 [0, focal_length, center[1]],
                 [0, 0, 1]], dtype="double"
            )

        # 1. å¾ MediaPipe æå–å°æ‡‰çš„ 6 å€‹ 2D é—œéµé»
        # æ³¨æ„ï¼šMediaPipe çš„é»æ˜¯æ­£è¦åŒ–çš„ (0~1)ï¼Œè¦ä¹˜ä¸Šå¯¬é«˜
        # Index: Nose=1, Chin=152, L_Eye=33, R_Eye=263, L_Mouth=61, R_Mouth=291
        points = [(face_landmarks.landmark[i].x*img_w, 
                   face_landmarks.landmark[i].y*img_h) for i in pose_landmark_index]
    
        #[
        #    (face_landmarks.landmark[1].x * img_w, face_landmarks.landmark[1].y * img_h),     # Nose tip
        #    (face_landmarks.landmark[152].x * img_w, face_landmarks.landmark[152].y * img_h), # Chin
        #    (face_landmarks.landmark[33].x * img_w, face_landmarks.landmark[33].y * img_h),   # Left Eye
        #    (face_landmarks.landmark[263].x * img_w, face_landmarks.landmark[263].y * img_h), # Right Eye
        #    (face_landmarks.landmark[61].x * img_w, face_landmarks.landmark[61].y * img_h),   # Left Mouth
        #    (face_landmarks.landmark[291].x * img_w, face_landmarks.landmark[291].y * img_h),  # Right Mouth
        #    # --- [æ–°å¢] ---
        #    #(face_landmarks.landmark[362].x * img_w, face_landmarks.landmark[362].y * img_h), # 7. å·¦å…§çœ¼è§’
        #    #(face_landmarks.landmark[133].x * img_w, face_landmarks.landmark[133].y * img_h), # 8. å³å…§çœ¼è§’
        #    #(face_landmarks.landmark[105].x * img_w, face_landmarks.landmark[105].y * img_h),  # 9. å·¦çœ‰å°¾ (é€™é»ç›¸å°ç©©å®š)
        #    #(face_landmarks.landmark[334].x * img_w, face_landmarks.landmark[334].y * img_h)  # 10. å³çœ‰å°¾
        #    ]
        image_points = np.array(points, dtype="double")

        # 2. å‘¼å« SolvePnP
        success, rotation_vector, translation_vector = cv2.solvePnP(
            self.model_points, 
            image_points, 
            self.cam_matrix, 
            self.dist_coeffs, 
            flags=cv2.SOLVEPNP_SQPNP
        )

        # 3. å°‡æ—‹è½‰å‘é‡è½‰æ›ç‚ºæ­æ‹‰è§’ (Euler Angles)
        #é€™éƒ¨åˆ†æ•¸å­¸æ¯”è¼ƒæ·±ï¼Œä¸»è¦æ˜¯æŠŠæ—‹è½‰çŸ©é™£è½‰æˆæˆ‘å€‘çœ‹å¾—æ‡‚çš„è§’åº¦
        rmat, jac = cv2.Rodrigues(rotation_vector)
        angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)

        # 4. æå– Pitch, Yaw, Roll
        # æ ¹æ“š OpenCV çš„åº§æ¨™ç³»å®šç¾©ï¼š
        # angles[0] = Pitch (æŠ¬é ­ä½é ­)
        # angles[1] = Yaw (å·¦å³è½‰)
        # angles[2] = Roll (æ­ªé ­)
    
        pitch = angles[0]  # è½‰æ›æ¯”ä¾‹å¾®èª¿ (è¦–éœ€æ±‚èª¿æ•´å¼·åº¦)
        yaw   = angles[1]
        roll  = angles[2] 
        if self.first == True:
            self.rt = rotation_vector,translation_vector
            self.first = False
            self.init_angle = yaw,pitch,roll
        iyaw,ipitch,iroll = self.init_angle
        pitch -= ipitch
        yaw -= iyaw
        roll -= iroll
        
        #pitch = max(-90, min(90, pitch))
        #yaw   = max(-90, min(90, yaw))
        #roll  = max(-90, min(90, roll))
        # é€™è£¡çš„æ•¸å€¼é€šå¸¸å¾ˆæ•æ„Ÿï¼Œå¯èƒ½éœ€è¦é™åˆ¶ç¯„åœ (Clamp)
        # é¡¯ç¤ºè¦–çª—
        debug = True
        if debug:
            debug_board = np.zeros((480, 640, 3), dtype=np.uint8)
            frontal_points = self.get_frontal_landmarks(rmat, face_landmarks,img_w,img_h)
            for point in frontal_points:
                cv2.circle(debug_board, point, 1, (0, 255, 0), -1) # ç•«ç¶ è‰²å°é»
            # ç•«å‡ºé¼»å°– (ç´…è‰²å¤§é») ä½œç‚ºåƒè€ƒä¸­å¿ƒ
            if len(frontal_points) > 1:
                for i in pose_landmark_index:
                 cv2.circle(debug_board, frontal_points[i], 3, (0, 0, 255), -1)
            cv2.imshow("Debug: Frontalized View", debug_board)

            #---------------------------------

            # 2. ç”¢ç”Ÿ Debug æ¯”è¼ƒç•«é¢
            # é€™è£¡æˆ‘å€‘åªæ¯”è¼ƒæœ‰åƒèˆ‡ PnP è¨ˆç®—çš„é‚£å¹¾å€‹å‰›æ€§é»
            # å‡è¨­ä½ ç”¨äº†ç‰¹å®š index çš„é»ï¼Œè¨˜å¾—è¦å°æ‡‰èµ·ä¾†

            # å¦‚æœä½  PnP ç”¨çš„é»å’Œå­˜ä¸‹ä¾†çš„ My_face é»æ•¸ä¸€æ¨£å¤šï¼Œç›´æ¥ç”¨ï¼š
            actual_points_list = []
            w = img_w
            h = img_h
            for lm in face_landmarks.landmark:
                px = lm.x * w
                py = lm.y * h
                pz = lm.z * w 
                actual_points_list.append([px, py, pz])

            points = [(p[0],p[1],p[2]) for p in my_face]
#            w = img_w
#            h = img_h
#            points = []
#            actual_points_list = []
#            for i in pose_landmark_index:
#                lm = face_landmarks.landmark[i]
#                px = lm.x * w
#                py = lm.y * h
#                pz = lm.z * w 
#                actual_points_list.append([px, py, pz])
#                points.append([my_face[i][0],my_face[i][1],my_face[i][2]])
#
            model_points = np.array(points,dtype=np.float64)
            #actual_points_list = apply_inverse_rigid_transform(
            #        actual_points_list, 
            #        rotation_vector, 
            #        translation_vector
            #    )
            actual_points_list = apply_inverse_rigid_transform(
                    actual_points_list, 
                    rotation_vector, 
                    translation_vector
                )

            comparison_image = draw_debug_comparison(
                model_points, 
                actual_points_list,
                win_size=600
            )
            cv2.imshow("Debug Comparison", comparison_image)
            #debug_board2 = np.zeros((480, 640, 3), dtype=np.uint8)
            #frontal_points = self.get_frontal_standard_landmarks(rmat,img_w,img_h)
            #for point in frontal_points:
            #    cv2.circle(debug_board2, point, 1, (0, 255, 0), -1) # ç•«ç¶ è‰²å°é»
            ## ç•«å‡ºé¼»å°– (ç´…è‰²å¤§é») ä½œç‚ºåƒè€ƒä¸­å¿ƒ
            #if len(frontal_points) > 1:
            #    for i in pose_landmark_index:
            #     cv2.circle(debug_board2, frontal_points[i], 3, (0, 0, 255), -1)
            #cv2.imshow("Debug: Frontalized View2", debug_board2)

            #debug_board3 = np.zeros((480, 640, 3), dtype=np.uint8)
            #frontal_pointsd = self.get_frontal_landmarks(rmat, face_landmarks,img_w,img_h)
            #for point in frontal_points:
            #    cv2.circle(debug_board3, point, 1, (0, 255, 0), -1) # ç•«ç¶ è‰²å°é»
            #for point in frontal_pointsd:
            #    cv2.circle(debug_board3, point, 1, (0, 255, 0), -1) # ç•«ç¶ è‰²å°é»

            ## ç•«å‡ºé¼»å°– (ç´…è‰²å¤§é») ä½œç‚ºåƒè€ƒä¸­å¿ƒ
            #if len(frontal_points) > 1:
            #    for i in pose_landmark_index:
            #     cv2.circle(debug_board3, frontal_points[i], 3, (0, 0, 255), -1)
            #     cv2.circle(debug_board3, frontal_pointsd[i], 3, (0, 0, 255), -1)
            #cv2.imshow("Debug: Frontalized View3", debug_board3)

            # é¡¯ç¤ºè¦–çª—
            cv2.waitKey(1)
        return yaw, pitch, roll
    def get_frontal_landmarks(self, rmat, face_landmarks, img_w, img_h):
        """
        è¼¸å…¥ï¼šç•¶å‰æ­ªæ–œçš„ landmarks å’Œæ—‹è½‰å‘é‡ rvec
        è¼¸å‡ºï¼šè¢«ã€Œè½‰æ­£ã€å¾Œçš„ 2D landmarks åº§æ¨™åˆ—è¡¨ (ç”¨æ–¼ç¹ªåœ–)
        """
        # 1. å–å¾—æ—‹è½‰çŸ©é™£ R
        #R = self.rt
        #R = self.rt
        
        #R = self.rt
        # 2. è¨ˆç®—é€†å‘æ—‹è½‰çŸ©é™£ (è½‰ç½®çŸ©é™£)
        # é€™å€‹çŸ©é™£çš„ä½œç”¨æ˜¯æŠŠæ­ªçš„é ­è½‰æ­£
        #R_inv = np.eye(3)

        # 3. æ”¶é›†ç•¶å‰æ‰€æœ‰ landmarks çš„ 3D åº§æ¨™
        # MediaPipe æä¾›çš„ z åº§æ¨™æ˜¯ç›¸å°æ·±åº¦ï¼Œæˆ‘å€‘éœ€è¦æŠŠå®ƒè®Šæˆé¡ä¼¼åƒç´ çš„å–®ä½
        landmarks_3d_list = []
        for lm in face_landmarks.landmark:
            # å°‡æ¨™æº–åŒ–åº§æ¨™è½‰æ›ç‚ºè¿‘ä¼¼çš„ 3D ç©ºé–“åº§æ¨™
            # x, y ä¹˜ä¸Šå¯¬é«˜ï¼Œz ä¹Ÿä¹˜ä¸Šå¯¬åº¦ä½œç‚ºæ·±åº¦æ¯”ä¾‹ä¼°è¨ˆ
            lx, ly, lz = lm.x * img_w, lm.y * img_h, lm.z * img_w
            landmarks_3d_list.append([lx, ly, lz])
            

        rotation_vector , translation_vector = self.rt
        points_np = np.array(landmarks_3d_list, dtype=np.float32)

        # Face Mesh Coordinate is not consistent
        #points_np = apply_inverse_rigid_transform(points_np,rotation_vector,translation_vector)

        #nose_tip_index = 1
        #nose_pos = points_np[nose_tip_index]
        #centered_points = points_np - nose_pos
        #centered_points = points_np

        # 5. ã€æ ¸å¿ƒæ•¸å­¸ã€‘æ‡‰ç”¨é€†å‘æ—‹è½‰
        # çŸ©é™£ä¹˜æ³•ï¼š Unrotated_P = R_inv * Centered_P
        # å› ç‚ºæˆ‘å€‘çš„ points æ˜¯ N x 3 çš„å½¢ç‹€ï¼Œè¦è½‰ç½®ä¸€ä¸‹æ‰èƒ½ç›¸ä¹˜
        # çµæœå½¢ç‹€æ˜¯ 3 x N
        # è½‰å›ä¾†è®Šæˆ N x 3
        unrotated_points_3d = points_np
        #unrotated_points_3d = centered_points
        #uv_points = project_perspective_auto_fit(unrotated_points_3d, img_w,img_h,fov_degrees=60)
        #uv_points = fit_points_to_window(uv_points,img_w,img_h)
        #return uv_points

        # 6. å°‡è½‰æ­£å¾Œçš„ 3D é»æŠ•å½±å› 2D ç•«é¢ä»¥ä¾¿ç¹ªè£½
        frontal_points_2d = []
        # è¨­å®šç¹ªè£½ç•«é¢çš„ä¸­å¿ƒé» (ä¾‹å¦‚æ”¾åœ¨ç•«é¢ä¸­é–“)
        draw_center_x, draw_center_y = img_w // 2, img_h // 2

        # A. æ‰¾å‡ºè½‰æ­£å¾Œçš„äººè‡‰é‚Šç•Œ (Bounding Box)
        # unrotated_points_3d[:, 0] æ˜¯æ‰€æœ‰é»çš„ X
        # unrotated_points_3d[:, 1] æ˜¯æ‰€æœ‰é»çš„ Y
        min_x = np.min(unrotated_points_3d[:, 0])
        max_x = np.max(unrotated_points_3d[:, 0])
        min_y = np.min(unrotated_points_3d[:, 1])
        max_y = np.max(unrotated_points_3d[:, 1])

        face_w = max_x - min_x
        face_h = max_y - min_y
        
        # B. è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹ (Scale)
        # æˆ‘å€‘å¸Œæœ›è‡‰çš„å¯¬åº¦ä½”ç•«é¢çš„ padding_ratio
        # æˆ–è€…æ˜¯é«˜åº¦ä½”ç•«é¢çš„ padding_ratio
        # å–å…©è€…ä¸­è¼ƒå°çš„å€¼ï¼Œç¢ºä¿ä¸æœƒè¶…å‡ºç•«é¢
        padding_ratio = 0.8
        scale_x = (img_w * padding_ratio) / face_w
        scale_y = (img_h * padding_ratio) / face_h
        final_scale = min(scale_x, scale_y)
        # C. è¨ˆç®—ä¸­å¿ƒåç§» (Centering)
        # æˆ‘å€‘è¦æŠŠè‡‰çš„ "å¹¾ä½•ä¸­å¿ƒ" ç§»åˆ° "è¦–çª—ä¸­å¿ƒ"
        face_center_x = (min_x + max_x) / 2
        face_center_y = (min_y + max_y) / 2
        
        window_center_x = img_w // 2
        window_center_y = img_h // 2
        
        frontal_points_2d = []
        for p3d in unrotated_points_3d:
            px = int((p3d[0] - face_center_x) * final_scale + window_center_x)
            py = int((p3d[1] - face_center_y) * final_scale + window_center_y)
            frontal_points_2d.append((px, py))
            
        return frontal_points_2d

    def get_frontal_standard_landmarks(self, rmat,img_w, img_h):
        """
        è¼¸å…¥ï¼šç•¶å‰æ­ªæ–œçš„ landmarks å’Œæ—‹è½‰å‘é‡ rvec
        è¼¸å‡ºï¼šè¢«ã€Œè½‰æ­£ã€å¾Œçš„ 2D landmarks åº§æ¨™åˆ—è¡¨ (ç”¨æ–¼ç¹ªåœ–)
        """
        R = rmat #np.eye(3)#rmat

        # standard --rmat--> target
        landmarks_3d_list = []
        for lm in my_face:
            # å°‡æ¨™æº–åŒ–åº§æ¨™è½‰æ›ç‚ºè¿‘ä¼¼çš„ 3D ç©ºé–“åº§æ¨™
            # x, y ä¹˜ä¸Šå¯¬é«˜ï¼Œz ä¹Ÿä¹˜ä¸Šå¯¬åº¦ä½œç‚ºæ·±åº¦æ¯”ä¾‹ä¼°è¨ˆ
            #lx, ly, lz = lm[0] * img_w, lm[1] * img_h, lm[2] * img_w
            lx, ly, lz = lm[0], lm[1] , lm[2]
            landmarks_3d_list.append([lx, ly, lz])
            
        points_np = np.array(landmarks_3d_list, dtype=np.float32)

        # 4. ã€é—œéµæ­¥é©Ÿã€‘å°‡åº§æ¨™ä¸­å¿ƒåŒ–
        # æ—‹è½‰æ˜¯ç¹è‘—åŸé»(0,0,0)è½‰çš„ã€‚æˆ‘å€‘å¿…é ˆæŠŠé¼»å°–ç§»åˆ°åŸé»ã€‚
        nose_tip_index = 1
        nose_pos = points_np[nose_tip_index]
        centered_points = points_np - nose_pos

        rotated_points_3d_T  = R @ centered_points.T
        rotated_points_3d  = rotated_points_3d_T.T

        uv_points = project_perspective_auto_fit(rotated_points_3d, img_w,img_h,fov_degrees=60)
        return uv_points


        ## 6. å°‡è½‰æ­£å¾Œçš„ 3D é»æŠ•å½±å› 2D ç•«é¢ä»¥ä¾¿ç¹ªè£½
        #frontal_points_2d = []
        ## è¨­å®šç¹ªè£½ç•«é¢çš„ä¸­å¿ƒé» (ä¾‹å¦‚æ”¾åœ¨ç•«é¢ä¸­é–“)
        #draw_center_x, draw_center_y = img_w // 2, img_h // 2

        ## A. æ‰¾å‡ºè½‰æ­£å¾Œçš„äººè‡‰é‚Šç•Œ (Bounding Box)
        ## unrotated_points_3d[:, 0] æ˜¯æ‰€æœ‰é»çš„ X
        ## unrotated_points_3d[:, 1] æ˜¯æ‰€æœ‰é»çš„ Y
        #min_x = np.min(rotated_points_3d[:, 0])
        #max_x = np.max(rotated_points_3d[:, 0])
        #min_y = np.min(rotated_points_3d[:, 1])
        #max_y = np.max(rotated_points_3d[:, 1])

        #face_w = max_x - min_x
        #face_h = max_y - min_y
        
        ## B. è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹ (Scale)
        ## æˆ‘å€‘å¸Œæœ›è‡‰çš„å¯¬åº¦ä½”ç•«é¢çš„ padding_ratio
        ## æˆ–è€…æ˜¯é«˜åº¦ä½”ç•«é¢çš„ padding_ratio
        ## å–å…©è€…ä¸­è¼ƒå°çš„å€¼ï¼Œç¢ºä¿ä¸æœƒè¶…å‡ºç•«é¢
        #padding_ratio = 0.8
        #scale_x = (img_w * padding_ratio) / face_w
        #scale_y = (img_h * padding_ratio) / face_h
        #final_scale = min(scale_x, scale_y)
        ## C. è¨ˆç®—ä¸­å¿ƒåç§» (Centering)
        ## æˆ‘å€‘è¦æŠŠè‡‰çš„ "å¹¾ä½•ä¸­å¿ƒ" ç§»åˆ° "è¦–çª—ä¸­å¿ƒ"
        #face_center_x = (min_x + max_x) / 2
        #face_center_y = (min_y + max_y) / 2
        
        #window_center_x = img_w // 2
        #window_center_y = img_h // 2
        
        #frontal_points_2d = []
        #for p3d in rotated_points_3d:
        #    px = int((p3d[0] - face_center_x) * final_scale + window_center_x)
        #    py = int((p3d[1] - face_center_y) * final_scale + window_center_y)
        #    frontal_points_2d.append((px, py))
        #    
        #return frontal_points_2d

    def release(self):
        self.cap.release()
class AsyncFaceTracker:
    """
    éåŒæ­¥è¿½è¹¤å™¨ï¼šç”¨ç¨ç«‹åŸ·è¡Œç·’è·‘ OpenCVï¼Œ
    ç¢ºä¿ä¸»è¦–çª—è¢« Windows å¡ä½æ™‚ (ä¾‹å¦‚æ‹–æ›³è¦–çª—)ï¼Œè¿½è¹¤ä¸æœƒä¸­æ–·ã€‚
    """
    def __init__(self):
        # å»ºç«‹åŸæœ¬çš„ Tracker
        self._tracker = FaceTracker()
        
        # å…±äº«è®Šæ•¸ (åŠ ä¸Š Lock é¿å…è®€å¯«è¡çªï¼Œé›–ç„¶ Python GIL æŸç¨®ç¨‹åº¦ä¸Šæœƒä¿è­·)
        self.lock = threading.Lock()
        self._current_iris_pos = (0.0, 0.0)
        self._current_blink_ratio = (1.0, 1.0)
        self._current_head_pose = (0.0, 0.0)
        
        self.running = True
        
        # å»ºç«‹ä¸¦å•Ÿå‹•åŸ·è¡Œç·’
        self.thread = threading.Thread(target=self._update_loop, daemon=True)
        self.thread.start()

    def _update_loop(self):
        """é€™æ˜¯èƒŒæ™¯åŸ·è¡Œç·’åœ¨åšçš„äº‹ï¼šä¸æ–·æ›´æ–°æ•¸æ“š"""
        while self.running:
            img = self._tracker.process()
            height, width, channels = img.shape
            # 1. å–å¾—æ•¸æ“š (é€™ä¸€æ­¥æœ€è€—æ™‚ï¼Œç¾åœ¨ä¸æœƒå¡ä½ UI äº†)
            dx, dy = self._tracker.get_iris_pos()
            bl, br = self._tracker.get_eye_blink_ratio()
            yaw, pitch,roll = self._tracker.get_head_pose(width,height)

            
            # 2. å­˜å…¥å…±äº«è®Šæ•¸
            with self.lock:
                self._current_iris_pos = (dx, dy)
                self._current_blink_ratio = (bl, br)
                self._current_head_pose = (yaw, pitch,roll)
            
            # ç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…åƒå…‰ CPU (ç´„ 60 FPS)
            time.sleep(0.016)

    # --- å¤–éƒ¨å‘¼å«çš„ä»‹é¢ (è®€å–æœ€æ–°å€¼) ---
    # é€™äº›å‡½å¼æœƒç”± Main Thread (UI) å‘¼å«ï¼Œé€Ÿåº¦æ¥µå¿«ï¼Œå› ç‚ºåªæ˜¯è®€è®Šæ•¸
    
    def get_iris_pos(self):
        with self.lock:
            return self._current_iris_pos

    def get_eye_blink_ratio(self):
        with self.lock:
            return self._current_blink_ratio

    def get_head_pose(self):
        with self.lock:
            return self._current_head_pose

    def release(self):
        self.running = False
        if self.thread.is_alive():
            self.thread.join()
        self._tracker.release()