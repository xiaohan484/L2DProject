import time
import threading
import numpy as np
import cv2
from pubsub import pub
import mediapipe as mp
import numpy as np
from Const import *
from ValueUtils import *

# pose_landmark_index = [1,152,33,263,61,291]
pose_landmark_index = [
    1,
    151,
    101,
    330,
    345,
    116,
    103,
    332,
    156,
    383,
    195,
    168,
    322,
    165,
    69,
    299,
]


def load_personal_model(json_path):
    """
    å¾ JSON è¼‰å…¥å€‹äººåŒ–è‡‰æ¨¡ï¼Œä¸¦æå–æŒ‡å®šçš„å‰›æ€§ç‰¹å¾µé»ã€‚

    Args:
        json_path (str): å‰›å‰›å­˜ä¸‹ä¾†çš„ json æª”æ¡ˆè·¯å¾‘
        target_indices (dict or list): ä½ éœ€è¦çš„å‰›æ€§é» Index (ä¾‹å¦‚ { 'NOSE': 1, ... })

    Returns:
        np.array: ç»™ solvePnP ç”¨çš„ model_points (N, 3)
    """
    try:
        print(f"ğŸ“‚ Loading personal model from {json_path}...")
        with open(json_path, "r") as f:
            all_landmarks = json.load(f)

        selected_points = []

        for pt in all_landmarks:
            selected_points.append(pt)

        # è½‰æˆ NumPy Float64 (solvePnP å¿…è¦æ ¼å¼)
        model_points_np = np.array(selected_points, dtype=np.float64)

        print(f"âœ… Loaded {len(model_points_np)} rigid points successfully.")
        return model_points_np

    except FileNotFoundError:
        print(f"âŒ Error: File {json_path} not found.")
        return None
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None


my_face = load_personal_model("assets/privacy/my_personal_landmarks.json")


def project_perspective_auto_fit(
    points_3d, win_w, win_h, fov_degrees=60.0, padding=0.1
):
    """
    é€è¦–æŠ•å½± + è‡ªå‹•ç¸®æ”¾ (Auto-Dolly)

    1. å°‡ 3D é»é›²ç§»åˆ°åŸé» (0,0,0)
    2. æ ¹æ“š FOV è¨ˆç®—è™›æ“¬ç„¦è·
    3. è¨ˆç®—éœ€è¦å°‡ç‰©é«”æ¨é å¤šå°‘è·é›¢ (tz)ï¼Œæ‰èƒ½å‰›å¥½å¡«æ»¿è¦–çª—
    4. åŸ·è¡Œé€è¦–æŠ•å½±: u = fx * (x / z) + cx

    Args:
        points_3d (np.array): (N, 3) 3D é»
        win_w, win_h (int): è¦–çª—å¤§å°
        fov_degrees (float): è™›æ“¬ç›¸æ©Ÿçš„è¦–é‡è§’åº¦ (é è¨­ 60 åº¦ï¼Œæ¥è¿‘äººçœ¼/Webcam)
        padding (float): é‚Šç·£ç•™ç™½æ¯”ä¾‹ (0.1 = 10%)

    Returns:
        np.array: (N, 2) æ•´æ•¸åº§æ¨™ (u, v)
    """
    # 1. è¤‡è£½è³‡æ–™ä»¥å…æ”¹åˆ°åŸå§‹æ•¸æ“š
    p3d = points_3d.copy()

    # 2. å¹¾ä½•ä¸­å¿ƒæ­¸é›¶ (Centering)
    # æ‰¾å‡ºç‰©é«”ä¸­å¿ƒï¼Œä¸¦å°‡æ‰€æœ‰é»ç§»åˆ°ä»¥ (0,0,0) ç‚ºä¸­å¿ƒ
    center = np.mean(p3d, axis=0)
    p3d_centered = p3d - center

    # 3. è¨ˆç®— 3D ç©ºé–“ä¸­çš„åŒ…åœç›’å°ºå¯¸ (Bounding Box Size)
    min_xyz = np.min(p3d_centered, axis=0)
    max_xyz = np.max(p3d_centered, axis=0)
    face_w_3d = max_xyz[0] - min_xyz[0]
    face_h_3d = max_xyz[1] - min_xyz[1]

    # 4. å»ºç«‹è™›æ“¬ç›¸æ©Ÿåƒæ•¸ (Intrinsics)
    # fx = (W / 2) / tan(fov / 2)
    fov_rad = np.radians(fov_degrees)
    focal_length = (win_w / 2) / np.tan(fov_rad / 2)

    cx = win_w / 2
    cy = win_h / 2

    # 5. è¨ˆç®—ã€Œæ¨è»Œè·é›¢ã€ (Dolly Distance / Z-Offset)
    # æˆ‘å€‘éœ€è¦å¤šæ·±çš„ Zï¼Œæ‰èƒ½è®“æŠ•å½±å¾Œçš„å¯¬åº¦ç­‰æ–¼è¦–çª—å¯¬åº¦ï¼Ÿ
    # å…¬å¼å°å‡º: projected_size = focal_length * (real_size / depth)
    # æ‰€ä»¥: depth = focal_length * real_size / projected_size

    target_w = win_w * (1.0 - padding * 2)
    target_h = win_h * (1.0 - padding * 2)

    # é¿å…é™¤ä»¥é›¶
    face_w_3d = max(face_w_3d, 1e-5)
    face_h_3d = max(face_h_3d, 1e-5)

    dist_w = focal_length * (face_w_3d / target_w)
    dist_h = focal_length * (face_h_3d / target_h)

    # å–æœ€é çš„è·é›¢ï¼Œç¢ºä¿å¯¬å’Œé«˜éƒ½å¡å¾—é€²å»
    z_offset = max(dist_w, dist_h)

    # ç¨å¾®åŠ ä¸€é»è¿‘å¹³é¢å‰ªè£ä¿è­· (Near Plane Clipping protection)
    # è®“è‡‰çš„æœ€å‡¸é»ä¸æœƒæˆ³åˆ°é¡é ­å¾Œé¢
    max_z_variation = np.max(np.abs(p3d_centered[:, 2]))
    z_dist = z_offset + max_z_variation + 10  # å®‰å…¨è·é›¢

    # 6. åŸ·è¡Œé€è¦–æŠ•å½± (Perspective Projection)
    # u = fx * x / z + cx
    # v = fy * y / z + cy

    # åŠ ä¸Šæ·±åº¦æ¨ç§»
    z_coords = p3d_centered[:, 2] + z_dist

    # æŠ•å½±é‹ç®—
    # æ³¨æ„: å¦‚æœä½ çš„ Y è»¸æ–¹å‘è·Ÿè¢å¹•ç›¸åï¼Œå¯ä»¥åœ¨é€™è£¡åŠ è² è™Ÿ (-p3d_centered[:, 1])
    # é€™è£¡å‡è¨­ Y å‘ä¸‹ç‚ºæ­£ (OpenCV æ¨™æº–)
    u = (focal_length * p3d_centered[:, 0] / z_coords) + cx
    v = (focal_length * p3d_centered[:, 1] / z_coords) + cy

    return np.stack((u, v), axis=1).astype(np.int32)


def fit_points_to_window(uv_points, window_w, window_h, padding_ratio=0.1):
    """
    æ¥æ”¶ä»»æ„ä¸€çµ„ UV é» (N, 2)ï¼Œè‡ªå‹•ç¸®æ”¾å¹³ç§»ä»¥å¡«æ»¿è¦–çª—ã€‚

    Args:
        uv_points (np.array): å½¢ç‹€ (N, 2) çš„é»ï¼Œå–®ä½ä¸é™ (å¯ä»¥æ˜¯ 0~1 æˆ–ä»»æ„ pixel)
        window_w (int): ç›®æ¨™è¦–çª—å¯¬åº¦
        window_h (int): ç›®æ¨™è¦–çª—é«˜åº¦
        padding_ratio (float): é‚Šç·£ç•™ç™½æ¯”ä¾‹ (ä¾‹å¦‚ 0.1 ä»£è¡¨ç•™ 10% ç©ºéš™)

    Returns:
        np.array: è½‰æ›å¾Œçš„ (N, 2) æ•´æ•¸åº§æ¨™ï¼Œå¯ç›´æ¥ç•«åœ–
    """
    points = np.array(uv_points)

    # 1. æ‰¾å‡ºç›®å‰çš„é‚Šç•Œ (Bounding Box)
    min_x, min_y = np.min(points, axis=0)
    max_x, max_y = np.max(points, axis=0)

    current_w = max_x - min_x
    current_h = max_y - min_y
    current_center_x = (min_x + max_x) / 2
    current_center_y = (min_y + max_y) / 2

    # 2. é¿å…é™¤ä»¥é›¶ (å¦‚æœåªæœ‰ä¸€å€‹é»æˆ–é»é‡ç–Š)
    current_w = max(current_w, 1e-5)
    current_h = max(current_h, 1e-5)

    # 3. è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹ (Scale to Fit)
    # æˆ‘å€‘å¸Œæœ›å¯¬åº¦èƒ½æ’æ»¿è¦–çª—ï¼Œé«˜åº¦ä¹Ÿèƒ½æ’æ»¿è¦–çª—
    # ä½†ç‚ºäº†ä¸è®Šå½¢ï¼Œæˆ‘å€‘å– "å…©è€…ä¸­è¼ƒå°" çš„ç¸®æ”¾å€ç‡
    scale_x = window_w / current_w
    scale_y = window_h / current_h

    # å¯¦éš›ç¸®æ”¾å€ç‡ (ä¹˜ä¸Š (1 - padding*2) æ˜¯ç‚ºäº†ç•™å‡ºé›™é‚Šçš„ç™½é‚Š)
    final_scale = min(scale_x, scale_y) * (1.0 - padding_ratio * 2)

    # 4. åŸ·è¡Œè®Šæ›å…¬å¼
    # New = (Old - Old_Center) * Scale + New_Window_Center

    target_center_x = window_w // 2
    target_center_y = window_h // 2

    new_xs = (points[:, 0] - current_center_x) * final_scale + target_center_x
    new_ys = (points[:, 1] - current_center_y) * final_scale + target_center_y

    # 5. çµ„åˆä¸¦è½‰æ•´æ•¸
    new_points = np.stack((new_xs, new_ys), axis=1).astype(np.int32)

    return new_points


class FaceTracker:
    def __init__(self):
        self.mp_face_mesh = mp.solutions.face_mesh
        # refine_landmarks=True æ˜¯é—œéµï¼Œé€™æ¨£æ‰æœƒå›å‚³ç³å­”(Iris)çš„åº§æ¨™
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
        )
        self.cap = cv2.VideoCapture(0)

        # --- [æ–°å¢] Head Pose Estimation éœ€è¦çš„åƒæ•¸ ---
        self.img_w = 640  # é è¨­ï¼Œä¹‹å¾Œæœƒå‹•æ…‹æ›´æ–°
        self.img_h = 480

        # å®šç¾©æ¨™æº– 3D è‡‰éƒ¨æ¨¡å‹çš„ 6 å€‹ç‰¹å¾µé» (ä¸–ç•Œåº§æ¨™)
        # é †åºï¼šé¼»å°–, ä¸‹å·´, å·¦çœ¼è§’, å³çœ¼è§’, å·¦å˜´è§’, å³å˜´è§’
        points = [
            (my_face[i][0], my_face[i][1], my_face[i][2]) for i in pose_landmark_index
        ]
        self.model_points = np.array(points, dtype=np.float64)
        # self.model_points = np.array([
        #    (0.0, 0.0, 0.0),             # Nose tip (åŸé»)
        #    (0.0, 330.0, 65.0),          # Chin (ä¸‹å·´åœ¨ä¸‹é¢ -> Yæ˜¯æ­£çš„)
        #    (-225.0, -170.0, 135.0),     # Left eye left corner (çœ¼ç›åœ¨ä¸Šé¢ -> Yæ˜¯è² çš„)
        #    (225.0, -170.0, 135.0),      # Right eye right corner
        #    (-150.0, 150.0, 125.0),      # Left Mouth corner (å˜´å·´åœ¨ä¸‹é¢ -> Yæ˜¯æ­£çš„)
        #    (150.0, 150.0, 125.0),        # Right Mouth corner

        #    ## --- [æ–°å¢] ç©©å®šéŒ¨é» ---
        #    #(-60.0, -170.0, 100.0),      # 7. Left Eye Inner Corner (å…§çœ¼è§’) - é é¼»æ¨‘
        #    #(60.0, -170.0, 100.0),       # 8. Right Eye Inner Corner (å…§çœ¼è§’) - é é¼»æ¨‘

        #    ## --- [ä¿®æ­£] 9, 10 çœ‰å³°/é¡é ­ (ä½ç½®è¼ƒé«˜ï¼Œä¸”ç¨å¾®æ·±ä¸€é») ---
        #    ## Y è¨­ç‚º 300.0 (æ¯”çœ¼ç› 170 é«˜)
        #    ## X è¨­ç‚º 180.0 (å¯¬åº¦é©ä¸­)
        #    #(-180.0, -300.0, 100.0),     # 9. å·¦çœ‰å³° (å°æ‡‰ 105)
        #    #(180.0, -300.0, 100.0)       # 10. å³çœ‰å³° (å°æ‡‰ 334)
        # ], dtype=np.float64)

        # ç›¸æ©ŸçŸ©é™£ (ä¹‹å¾Œåœ¨ process è£¡åˆå§‹åŒ–ä¸€æ¬¡å³å¯)
        self.cam_matrix = None
        self.dist_coeffs = np.zeros((4, 1))  # å‡è¨­ç„¡é¡é ­è®Šå½¢.VideoCapture(0)
        self.first = True

    def isFake(self):
        return False

    def process(self):
        success, image = self.cap.read()
        image.flags.writeable = False
        cv2.imshow("tracking result", image)
        cv2.waitKey(1)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        self.results = self.face_mesh.process(image)
        self.image = image
        return image

    def get_iris_pos(self):
        """
        å›å‚³çœ¼çƒçš„ç›¸å°ä½ç½® (x, y)
        x: -1.0 (å·¦) ~ 1.0 (å³), 0.0 æ˜¯ä¸­é–“
        y: -1.0 (ä¸Š) ~ 1.0 (ä¸‹), 0.0 æ˜¯ä¸­é–“
        """
        results = self.results
        dx, dy = 0, 0
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark

            # --- æ ¸å¿ƒæ¼”ç®—æ³•ï¼šè¨ˆç®—ç³å­”åœ¨çœ¼æ¡†ä¸­çš„ç›¸å°ä½ç½® ---
            # æˆ‘å€‘ä½¿ç”¨å·¦çœ¼ä¾†åšåŸºæº– (MediaPipe çš„å·¦çœ¼å°æ‡‰åˆ°ç•«é¢å³é‚Šçš„è‡‰)
            # ç´¢å¼•: 33(çœ¼é ­), 133(çœ¼å°¾), 468(ç³å­”ä¸­å¿ƒ)

            # å–å¾—åº§æ¨™ (åªè¦ x, y)
            in_x, in_y = landmarks[33].x, landmarks[33].y  # Inner Corner
            out_x, out_y = landmarks[133].x, landmarks[133].y  # Outer Corner
            iris_x, iris_y = landmarks[468].x, landmarks[468].y  # Iris Center

            # 1. è¨ˆç®—çœ¼å¯¬å’Œçœ¼é«˜ (å¤§ç•¥ä¼°ç®—)
            eye_width = out_x - in_x
            # ç‚ºäº†ç°¡åŒ–ï¼Œé«˜åº¦æˆ‘å€‘å…ˆç”¨çœ¼å¯¬çš„ä¸€å€‹æ¯”ä¾‹ä¾†æŠ“ï¼Œæˆ–è€…æš«æ™‚å¿½ç•¥ Y è»¸ç²¾ç¢ºåº¦

            # 2. è¨ˆç®—ç³å­”ç›¸å°æ–¼çœ¼é ­çš„è·é›¢
            dist_x = iris_x - in_x

            # 3. ç®—å‡ºæ¯”ä¾‹ (0.0 ~ 1.0)
            # 0.5 ä»£è¡¨åœ¨ä¸­é–“ã€‚ä¸€èˆ¬ä¾†èªªç³å­”ä¸æœƒç¢°åˆ°çœ¼è§’ï¼Œæ‰€ä»¥ç¯„åœå¤§æ¦‚åœ¨ 0.3~0.7 ä¹‹é–“
            ratio_x = dist_x / eye_width

            # 4. æ­£è¦åŒ–åˆ° -1 ~ 1 (ç‚ºäº†çµ¦ Pygame ç”¨)
            # æˆ‘å€‘å‡è¨­ 0.5 æ˜¯ä¸­å¿ƒé»ã€‚
            # (ratio - 0.5) * 2 -> è®Šæˆ -1 ~ 1
            # ä¹˜ä¸Šä¸€å€‹æ•æ„Ÿåº¦ä¿‚æ•¸ (Sensitivity)ï¼Œè®“çœ¼ç›å‹•å¾—æ›´éˆæ•
            SENSITIVITY = 2.5
            dx = (ratio_x - 0.45) * 2 * SENSITIVITY  # 0.45 æ˜¯ç¨å¾®ä¿®æ­£ä¸­å¿ƒåç§»

            # Y è»¸ç°¡å–®è™•ç† (çœ¼çƒä¸Šä¸‹)
            # ä½¿ç”¨çœ¼çš®ä¸Šä¸‹é»ï¼š159 (ä¸Š), 145 (ä¸‹)
            top_y = landmarks[159].y
            bot_y = landmarks[145].y
            eye_height = bot_y - top_y
            dist_y = iris_y - top_y
            ratio_y = dist_y / eye_height
            dy = (ratio_y - 0.45) * 2 * SENSITIVITY

            # é™åˆ¶æ•¸å€¼åœ¨ -1 ~ 1 ä¹‹é–“ (Clamping)
            dx = -max(-1.0, min(1.0, dx))
            dy = -max(-1.0, min(1.0, dy))

        return dx, dy

    def calculate_mouth_openness(self, image_width, image_height):
        """
        è¨ˆç®—å˜´å·´å¼µé–‹æ¯”ä¾‹ (MAR - Mouth Aspect Ratio)

        Args:
            landmarks: MediaPipe è¿”å›çš„ normalized_landmarks (åŒ…å« x, y)
            image_width: ç•«å¸ƒå¯¬åº¦ (ç”¨æ–¼é‚„åŸåº§æ¨™)
            image_height: ç•«å¸ƒé«˜åº¦

        Returns:
            float: åŸå§‹ MAR æ•¸å€¼ (é€šå¸¸åœ¨ 0.0 ~ 0.5 ä¹‹é–“)
        """
        results = self.results
        if results.multi_face_landmarks is None:
            return
        landmarks = results.multi_face_landmarks[0].landmark
        dx, dy = 0, 0
        # 1. å®šç¾©é—œéµé»ç´¢å¼• (Inner Lips)
        IDX_TOP = 13
        IDX_BOTTOM = 14
        IDX_LEFT = 61
        IDX_RIGHT = 291
        # 2. å–å¾—åº§æ¨™é» (å°‡ Normalized 0~1 è½‰ç‚º Pixel åº§æ¨™)
        # æ³¨æ„ï¼šå¦‚æœä¸è½‰ Pixel ç›´æ¥ç”¨ Normalized ç®—ä¹Ÿå¯ä»¥ï¼Œä½†è½‰ Pixel æ¯”è¼ƒç›´è§€å¥½é™¤éŒ¯
        p_top = np.array(
            [landmarks[IDX_TOP].x * image_width, landmarks[IDX_TOP].y * image_height]
        )
        p_bottom = np.array(
            [
                landmarks[IDX_BOTTOM].x * image_width,
                landmarks[IDX_BOTTOM].y * image_height,
            ]
        )
        p_left = np.array(
            [landmarks[IDX_LEFT].x * image_width, landmarks[IDX_LEFT].y * image_height]
        )
        p_right = np.array(
            [
                landmarks[IDX_RIGHT].x * image_width,
                landmarks[IDX_RIGHT].y * image_height,
            ]
        )

        # 3. è¨ˆç®—æ­å¹¾é‡Œå¾—è·é›¢ (Euclidean Distance)
        # å‚ç›´è·é›¢ (é–‹åˆç¨‹åº¦)
        height = np.linalg.norm(p_top - p_bottom)

        # æ°´å¹³è·é›¢ (å˜´å·´å¯¬åº¦ - ä½œç‚ºåˆ†æ¯)
        width = np.linalg.norm(p_left - p_right)

        # 4. é˜²å‘†æ©Ÿåˆ¶ (é¿å…é™¤ä»¥é›¶)
        if width < 1e-6:
            return 0.0

        # 5. è¨ˆç®—æ¯”ä¾‹
        mar = height / width
        mar = map_range(mar, 0, 0.5, 0, 1)
        return mar

    def get_eye_blink_ratio(self):
        """
        è¨ˆç®—å·¦å³çœ¼çš„é–‹é—”ç¨‹åº¦ (Blink Ratio)
        å›å‚³: (left_ratio, right_ratio)
        æ•¸å€¼é€šå¸¸åœ¨ 0.0 (é–‰) ~ 0.3 (å¤§é–‹) ä¹‹é–“
        """
        results = self.results
        left_ratio = 1.0
        right_ratio = 1.0

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark

            # --- å·¦çœ¼é—œéµé» (MediaPipe çš„å·¦çœ¼å°æ‡‰ç•«é¢å³å´) ---
            # å‚ç›´: 159 (ä¸Š), 145 (ä¸‹)
            # æ°´å¹³: 33 (å…§), 133 (å¤–)
            l_top = landmarks[159].y
            l_bot = landmarks[145].y
            l_in = landmarks[33].x
            l_out = landmarks[133].x

            # è¨ˆç®—å‚ç›´è·é›¢ / æ°´å¹³è·é›¢ (æ¨™æº–åŒ–ï¼Œé¿å…é›¢é¡é ­é è¿‘å½±éŸ¿æ•¸å€¼)
            # åŠ ä¸Šä¸€å€‹æ¥µå°çš„æ•¸ 1e-6 é¿å…é™¤ä»¥é›¶
            left_ratio = abs(l_bot - l_top) / (abs(l_out - l_in) + 1e-6)

            # --- å³çœ¼é—œéµé» ---
            # å‚ç›´: 386 (ä¸Š), 374 (ä¸‹)
            # æ°´å¹³: 362 (å…§), 263 (å¤–)
            r_top = landmarks[386].y
            r_bot = landmarks[374].y
            r_in = landmarks[362].x
            r_out = landmarks[263].x

            right_ratio = abs(r_bot - r_top) / (abs(r_out - r_in) + 1e-6)

        return left_ratio, right_ratio

    def get_head_pose(self, img_w, img_h):
        """
        è¨ˆç®—é ­éƒ¨å§¿æ…‹ (Yaw, Pitch, Roll)
        å›å‚³: yaw, pitch, roll (å–®ä½: åº¦ degree)
        """
        results = self.results
        if results.multi_face_landmarks:
            face_landmarks = results.multi_face_landmarks[0]
        else:
            return (0, 0, 0)
        self.img_w = img_w
        self.img_h = img_h

        # å¦‚æœé‚„æ²’è¨­å®šç›¸æ©ŸçŸ©é™£ï¼Œè¨­ä¸€å€‹ä¼°è¨ˆå€¼
        if self.cam_matrix is None:
            focal_length = img_w
            center = (img_w / 2, img_h / 2)
            self.cam_matrix = np.array(
                [[focal_length, 0, center[0]], [0, focal_length, center[1]], [0, 0, 1]],
                dtype="double",
            )

        # 1. å¾ MediaPipe æå–å°æ‡‰çš„ 6 å€‹ 2D é—œéµé»
        # æ³¨æ„ï¼šMediaPipe çš„é»æ˜¯æ­£è¦åŒ–çš„ (0~1)ï¼Œè¦ä¹˜ä¸Šå¯¬é«˜
        # Index: Nose=1, Chin=152, L_Eye=33, R_Eye=263, L_Mouth=61, R_Mouth=291
        points = [
            (face_landmarks.landmark[i].x * img_w, face_landmarks.landmark[i].y * img_h)
            for i in pose_landmark_index
        ]

        # [
        #    (face_landmarks.landmark[1].x * img_w, face_landmarks.landmark[1].y * img_h),     # Nose tip
        #    (face_landmarks.landmark[152].x * img_w, face_landmarks.landmark[152].y * img_h), # Chin
        #    (face_landmarks.landmark[33].x * img_w, face_landmarks.landmark[33].y * img_h),   # Left Eye
        #    (face_landmarks.landmark[263].x * img_w, face_landmarks.landmark[263].y * img_h), # Right Eye
        #    (face_landmarks.landmark[61].x * img_w, face_landmarks.landmark[61].y * img_h),   # Left Mouth
        #    (face_landmarks.landmark[291].x * img_w, face_landmarks.landmark[291].y * img_h),  # Right Mouth
        #    # --- [æ–°å¢] ---
        #    #(face_landmarks.landmark[362].x * img_w, face_landmarks.landmark[362].y * img_h), # 7. å·¦å…§çœ¼è§’
        #    #(face_landmarks.landmark[133].x * img_w, face_landmarks.landmark[133].y * img_h), # 8. å³å…§çœ¼è§’
        #    #(face_landmarks.landmark[105].x * img_w, face_landmarks.landmark[105].y * img_h),  # 9. å·¦çœ‰å°¾ (é€™é»ç›¸å°ç©©å®š)
        #    #(face_landmarks.landmark[334].x * img_w, face_landmarks.landmark[334].y * img_h)  # 10. å³çœ‰å°¾
        #    ]
        image_points = np.array(points, dtype="double")

        # 2. å‘¼å« SolvePnP
        success, rotation_vector, translation_vector = cv2.solvePnP(
            self.model_points,
            image_points,
            self.cam_matrix,
            self.dist_coeffs,
            flags=cv2.SOLVEPNP_SQPNP,
        )

        # 3. å°‡æ—‹è½‰å‘é‡è½‰æ›ç‚ºæ­æ‹‰è§’ (Euler Angles)
        # é€™éƒ¨åˆ†æ•¸å­¸æ¯”è¼ƒæ·±ï¼Œä¸»è¦æ˜¯æŠŠæ—‹è½‰çŸ©é™£è½‰æˆæˆ‘å€‘çœ‹å¾—æ‡‚çš„è§’åº¦
        rmat, jac = cv2.Rodrigues(rotation_vector)
        angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)

        # 4. æå– Pitch, Yaw, Roll
        # æ ¹æ“š OpenCV çš„åº§æ¨™ç³»å®šç¾©ï¼š
        # angles[0] = Pitch (æŠ¬é ­ä½é ­)
        # angles[1] = Yaw (å·¦å³è½‰)
        # angles[2] = Roll (æ­ªé ­)

        pitch = angles[0]  # è½‰æ›æ¯”ä¾‹å¾®èª¿ (è¦–éœ€æ±‚èª¿æ•´å¼·åº¦)
        yaw = angles[1]
        roll = angles[2]
        if self.first:
            self.first = False
            self.init_angle = yaw, pitch, roll
        iyaw, ipitch, iroll = self.init_angle
        pitch -= ipitch
        yaw -= iyaw
        roll -= iroll

        # pitch = max(-90, min(90, pitch))
        # yaw   = max(-90, min(90, yaw))
        # roll  = max(-90, min(90, roll))
        # é€™è£¡çš„æ•¸å€¼é€šå¸¸å¾ˆæ•æ„Ÿï¼Œå¯èƒ½éœ€è¦é™åˆ¶ç¯„åœ (Clamp)
        # é¡¯ç¤ºè¦–çª—
        debug = True
        if debug:
            debug_board = np.zeros((480, 640, 3), dtype=np.uint8)
            frontal_points = self.get_frontal_landmarks(
                rmat, face_landmarks, img_w, img_h
            )
            for point in frontal_points:
                cv2.circle(debug_board, point, 1, (0, 255, 0), -1)  # ç•«ç¶ è‰²å°é»
            # ç•«å‡ºé¼»å°– (ç´…è‰²å¤§é») ä½œç‚ºåƒè€ƒä¸­å¿ƒ
            if len(frontal_points) > 1:
                for i in pose_landmark_index:
                    cv2.circle(debug_board, frontal_points[i], 3, (0, 0, 255), -1)
            cv2.imshow("Debug: Frontalized View", debug_board)
            # é¡¯ç¤ºè¦–çª—
            cv2.waitKey(1)
        return yaw, pitch, roll

    def get_frontal_landmarks(self, rmat, face_landmarks, img_w, img_h):
        """
        è¼¸å…¥ï¼šç•¶å‰æ­ªæ–œçš„ landmarks å’Œæ—‹è½‰å‘é‡ rvec
        è¼¸å‡ºï¼šè¢«ã€Œè½‰æ­£ã€å¾Œçš„ 2D landmarks åº§æ¨™åˆ—è¡¨ (ç”¨æ–¼ç¹ªåœ–)
        """
        # 1. å–å¾—æ—‹è½‰çŸ©é™£ R
        # R = self.rt
        # R = self.rt

        # R = self.rt
        # 2. è¨ˆç®—é€†å‘æ—‹è½‰çŸ©é™£ (è½‰ç½®çŸ©é™£)
        # é€™å€‹çŸ©é™£çš„ä½œç”¨æ˜¯æŠŠæ­ªçš„é ­è½‰æ­£
        # R_inv = np.eye(3)

        # 3. æ”¶é›†ç•¶å‰æ‰€æœ‰ landmarks çš„ 3D åº§æ¨™
        # MediaPipe æä¾›çš„ z åº§æ¨™æ˜¯ç›¸å°æ·±åº¦ï¼Œæˆ‘å€‘éœ€è¦æŠŠå®ƒè®Šæˆé¡ä¼¼åƒç´ çš„å–®ä½
        landmarks_3d_list = []
        for lm in face_landmarks.landmark:
            # å°‡æ¨™æº–åŒ–åº§æ¨™è½‰æ›ç‚ºè¿‘ä¼¼çš„ 3D ç©ºé–“åº§æ¨™
            # x, y ä¹˜ä¸Šå¯¬é«˜ï¼Œz ä¹Ÿä¹˜ä¸Šå¯¬åº¦ä½œç‚ºæ·±åº¦æ¯”ä¾‹ä¼°è¨ˆ
            lx, ly, lz = lm.x * img_w, lm.y * img_h, lm.z * img_w
            landmarks_3d_list.append([lx, ly, lz])

        points_np = np.array(landmarks_3d_list, dtype=np.float32)
        unrotated_points_3d = points_np
        frontal_points_2d = []

        min_x = np.min(unrotated_points_3d[:, 0])
        max_x = np.max(unrotated_points_3d[:, 0])
        min_y = np.min(unrotated_points_3d[:, 1])
        max_y = np.max(unrotated_points_3d[:, 1])

        face_w = max_x - min_x
        face_h = max_y - min_y

        # B. è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹ (Scale)
        # æˆ‘å€‘å¸Œæœ›è‡‰çš„å¯¬åº¦ä½”ç•«é¢çš„ padding_ratio
        # æˆ–è€…æ˜¯é«˜åº¦ä½”ç•«é¢çš„ padding_ratio
        # å–å…©è€…ä¸­è¼ƒå°çš„å€¼ï¼Œç¢ºä¿ä¸æœƒè¶…å‡ºç•«é¢
        padding_ratio = 0.8
        scale_x = (img_w * padding_ratio) / face_w
        scale_y = (img_h * padding_ratio) / face_h
        final_scale = min(scale_x, scale_y)
        # C. è¨ˆç®—ä¸­å¿ƒåç§» (Centering)
        # æˆ‘å€‘è¦æŠŠè‡‰çš„ "å¹¾ä½•ä¸­å¿ƒ" ç§»åˆ° "è¦–çª—ä¸­å¿ƒ"
        face_center_x = (min_x + max_x) / 2
        face_center_y = (min_y + max_y) / 2

        window_center_x = img_w // 2
        window_center_y = img_h // 2

        frontal_points_2d = []
        for p3d in unrotated_points_3d:
            px = int((p3d[0] - face_center_x) * final_scale + window_center_x)
            py = int((p3d[1] - face_center_y) * final_scale + window_center_y)
            frontal_points_2d.append((px, py))

        return frontal_points_2d

    def release(self):
        self.cap.release()


class AsyncFaceTracker:
    """
    éåŒæ­¥è¿½è¹¤å™¨ï¼šç”¨ç¨ç«‹åŸ·è¡Œç·’è·‘ OpenCVï¼Œ
    ç¢ºä¿ä¸»è¦–çª—è¢« Windows å¡ä½æ™‚ (ä¾‹å¦‚æ‹–æ›³è¦–çª—)ï¼Œè¿½è¹¤ä¸æœƒä¸­æ–·ã€‚
    """

    def __init__(self):
        # å»ºç«‹åŸæœ¬çš„ Tracker
        self._tracker = FaceTracker()

        # å…±äº«è®Šæ•¸ (åŠ ä¸Š Lock é¿å…è®€å¯«è¡çªï¼Œé›–ç„¶ Python GIL æŸç¨®ç¨‹åº¦ä¸Šæœƒä¿è­·)
        self.lock = threading.Lock()
        self._current_iris_pos = (0.0, 0.0)
        self._current_blink_ratio = (1.0, 1.0)
        self._current_head_pose = (0.0, 0.0)

        self.running = True

        # å»ºç«‹ä¸¦å•Ÿå‹•åŸ·è¡Œç·’
        self.thread = threading.Thread(target=self._update_loop, daemon=True)
        self.thread.start()

    def _update_loop(self):
        """é€™æ˜¯èƒŒæ™¯åŸ·è¡Œç·’åœ¨åšçš„äº‹ï¼šä¸æ–·æ›´æ–°æ•¸æ“š"""
        while self.running:
            img = self._tracker.process()
            height, width, channels = img.shape
            # 1. å–å¾—æ•¸æ“š (é€™ä¸€æ­¥æœ€è€—æ™‚ï¼Œç¾åœ¨ä¸æœƒå¡ä½ UI äº†)
            yaw, pitch, roll = self._tracker.get_head_pose(width, height)

            bl, br = self._tracker.get_eye_blink_ratio()
            dx, dy = self._tracker.get_iris_pos()
            mo = self._tracker.calculate_mouth_openness(width, height)

            # 2. å­˜å…¥å…±äº«è®Šæ•¸
            with self.lock:
                pub.sendMessage("MouthOpenness", openness_value=mo)
                pub.sendMessage(
                    "EyeInfo", info={"Pupils_Pos": (dx, dy), "Blinking": (bl, br)}
                )

                self._current_iris_pos = (dx, dy)
                self._current_blink_ratio = (bl, br)
                self._current_head_pose = (yaw, pitch, roll)
                self._current_mouth_openness = mo

            # ç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…åƒå…‰ CPU (ç´„ 60 FPS)
            time.sleep(0.016)

    # --- å¤–éƒ¨å‘¼å«çš„ä»‹é¢ (è®€å–æœ€æ–°å€¼) ---
    # é€™äº›å‡½å¼æœƒç”± Main Thread (UI) å‘¼å«ï¼Œé€Ÿåº¦æ¥µå¿«ï¼Œå› ç‚ºåªæ˜¯è®€è®Šæ•¸

    def get_iris_pos(self):
        with self.lock:
            return self._current_iris_pos

    def get_eye_blink_ratio(self):
        with self.lock:
            return self._current_blink_ratio

    def get_mouth_openness(self):
        with self.lock:
            return self._current_mouth_openness

    def get_head_pose(self):
        with self.lock:
            return self._current_head_pose

    def release(self):
        self.running = False
        if self.thread.is_alive():
            self.thread.join()
        self._tracker.release()
