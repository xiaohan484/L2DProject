import time
import threading
import numpy as np
import cv2
from pubsub import pub
import mediapipe as mp
import numpy as np
from Const import *
from ValueUtils import *
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# pose_landmark_index = [1,152,33,263,61,291]
pose_landmark_index = [
    1,
    151,
    101,
    330,
    345,
    116,
    103,
    332,
    156,
    383,
    195,
    168,
    322,
    165,
    69,
    299,
]


def load_personal_model(json_path):
    """
    å¾ JSON è¼‰å…¥å€‹äººåŒ–è‡‰æ¨¡ï¼Œä¸¦æå–æŒ‡å®šçš„å‰›æ€§ç‰¹å¾µé»ã€‚

    Args:
        json_path (str): å‰›å‰›å­˜ä¸‹ä¾†çš„ json æª”æ¡ˆè·¯å¾‘
        target_indices (dict or list): ä½ éœ€è¦çš„å‰›æ€§é» Index (ä¾‹å¦‚ { 'NOSE': 1, ... })

    Returns:
        np.array: ç»™ solvePnP ç”¨çš„ model_points (N, 3)
    """
    try:
        print(f"ğŸ“‚ Loading personal model from {json_path}...")
        with open(json_path, "r") as f:
            all_landmarks = json.load(f)

        selected_points = []

        for pt in all_landmarks:
            selected_points.append(pt)

        # è½‰æˆ NumPy Float64 (solvePnP å¿…è¦æ ¼å¼)
        model_points_np = np.array(selected_points, dtype=np.float64)

        print(f"âœ… Loaded {len(model_points_np)} rigid points successfully.")
        return model_points_np

    except FileNotFoundError:
        print(f"âŒ Error: File {json_path} not found.")
        return None
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None


my_face = load_personal_model("assets/privacy/my_personal_landmarks.json")


def convert_blendshape_dict(raw_shape):
    # ã€é—œéµæ­¥é©Ÿã€‘ç”¨ Comprehension è½‰æˆå­—å…¸ { 'jawOpen': 0.9, 'eyeBlink_L': 0.0 ... }
    blendshape_dict = {shape.category_name: shape.score for shape in raw_shape}
    return blendshape_dict


import cv2
import numpy as np


def get_reprojection_error(
    model_points, image_points, rvec, tvec, camera_matrix, dist_coeffs
):
    """
    è¨ˆç®— MediaPipe åœ°æ¨™çš„é‡æŠ•å½±èª¤å·®ï¼Œç”¨æ–¼åµæ¸¬é®æ“‹å°è‡´çš„å¹¾ä½•ç•¸è®Šã€‚
    (Comment: Calculate the reprojection error to detect facial distortion.)
    """
    projected_points, _ = cv2.projectPoints(
        model_points, rvec, tvec, camera_matrix, dist_coeffs
    )
    projected_points = projected_points.reshape(-1, 2)
    # è¨ˆç®—å¹³å‡æ¯å€‹é»çš„åƒç´ è·é›¢å·®
    error = np.mean(np.linalg.norm(image_points - projected_points, axis=1))

    return error


class FaceTracker:
    def __init__(self):
        self.mp_face_mesh = mp.solutions.face_mesh
        # refine_landmarks=True æ˜¯é—œéµï¼Œé€™æ¨£æ‰æœƒå›å‚³ç³å­”(Iris)çš„åº§æ¨™
        base_options = python.BaseOptions(
            model_asset_path="mediapipe_model/face_landmarker.task"
        )
        options = vision.FaceLandmarkerOptions(
            base_options=base_options,
            running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,
            min_face_presence_confidence=0.8,
            min_tracking_confidence=0.7,
            output_face_blendshapes=True,
            output_facial_transformation_matrixes=False,
            num_faces=1,
            result_callback=self.store_result,
        )
        self.detector = vision.FaceLandmarker.create_from_options(options)
        # self.face_mesh = self.mp_face_mesh.FaceMesh(
        #    max_num_faces=1,
        #    refine_landmarks=True,
        #    min_detection_confidence=0.5,
        #    min_tracking_confidence=0.5,
        # )
        self.cap = cv2.VideoCapture(0)

        # --- [æ–°å¢] Head Pose Estimation éœ€è¦çš„åƒæ•¸ ---
        self.img_w = 640  # é è¨­ï¼Œä¹‹å¾Œæœƒå‹•æ…‹æ›´æ–°
        self.img_h = 480

        # å®šç¾©æ¨™æº– 3D è‡‰éƒ¨æ¨¡å‹çš„ 6 å€‹ç‰¹å¾µé» (ä¸–ç•Œåº§æ¨™)
        # é †åºï¼šé¼»å°–, ä¸‹å·´, å·¦çœ¼è§’, å³çœ¼è§’, å·¦å˜´è§’, å³å˜´è§’
        points = [
            (my_face[i][0], my_face[i][1], my_face[i][2]) for i in pose_landmark_index
        ]
        self.model_points = np.array(points, dtype=np.float64)

        # ç›¸æ©ŸçŸ©é™£ (ä¹‹å¾Œåœ¨ process è£¡åˆå§‹åŒ–ä¸€æ¬¡å³å¯)
        self.cam_matrix = None
        self.dist_coeffs = np.zeros((4, 1))  # å‡è¨­ç„¡é¡é ­è®Šå½¢.VideoCapture(0)
        self.first = True
        self.results = None
        self.last_angle = (0, 0, 0)
        self.blendshapes = None

    def store_result(
        self,
        result: vision.FaceLandmarkerResult,
        output_image: mp.Image,
        timestamp_ms: int,
    ):
        # Store result for main loop to access
        self.results = result
        if len(self.results.face_blendshapes) > 0:
            self.blendshapes = convert_blendshape_dict(self.results.face_blendshapes[0])
        return

    def process(self):
        success, image = self.cap.read()
        image.flags.writeable = False
        cv2.imshow("tracking result", image)
        cv2.waitKey(1)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # self.results = self.face_mesh.process(image)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)
        frame_timestamp_ms = int(time.time() * 1000)
        self.detector.detect_async(mp_image, frame_timestamp_ms)
        self.image = image
        return image

    def get_iris_pos(self):
        """
        å›å‚³çœ¼çƒçš„ç›¸å°ä½ç½® (x, y)
        x: -1.0 (å·¦) ~ 1.0 (å³), 0.0 æ˜¯ä¸­é–“
        y: -1.0 (ä¸Š) ~ 1.0 (ä¸‹), 0.0 æ˜¯ä¸­é–“
        """
        if self.blendshapes is None:
            return

        dx = self.blendshapes["eyeLookOutLeft"] - self.blendshapes["eyeLookInLeft"]
        dy = self.blendshapes["eyeLookUpLeft"] - self.blendshapes["eyeLookDownLeft"]
        return -dx, -dy * 1.2

    def calculate_mouth_openness(self):
        """
        è¨ˆç®—å˜´å·´å¼µé–‹æ¯”ä¾‹ (MAR - Mouth Aspect Ratio)

        Args:
            landmarks: MediaPipe è¿”å›çš„ normalized_landmarks (åŒ…å« x, y)
            image_width: ç•«å¸ƒå¯¬åº¦ (ç”¨æ–¼é‚„åŸåº§æ¨™)
            image_height: ç•«å¸ƒé«˜åº¦

        Returns:
            float: åŸå§‹ MAR æ•¸å€¼ (é€šå¸¸åœ¨ 0.0 ~ 0.5 ä¹‹é–“)
        """
        if self.blendshapes is None:
            return 0
        jawOpen = self.blendshapes["jawOpen"]
        jawOpen = map_range(jawOpen, 0, 0.3, 0, 1)
        return jawOpen

    def get_eye_blink_ratio(self):
        """
        è¨ˆç®—å·¦å³çœ¼çš„é–‹é—”ç¨‹åº¦ (Blink Ratio)
        å›å‚³: (left_ratio, right_ratio)
        æ•¸å€¼é€šå¸¸åœ¨ 0.0 (é–‰) ~ 0.3 (å¤§é–‹) ä¹‹é–“
        """
        if self.blendshapes is None:
            return
        return (
            1 - self.blendshapes["eyeBlinkLeft"],
            1 - self.blendshapes["eyeBlinkRight"],
        )

    def get_head_pose(self, img_w, img_h):
        """
        è¨ˆç®—é ­éƒ¨å§¿æ…‹ (Yaw, Pitch, Roll)
        å›å‚³: yaw, pitch, roll (å–®ä½: åº¦ degree)
        """
        results = self.results
        if results.face_landmarks:
            face_landmarks = results.face_landmarks[0]
        else:
            return (0, 0, 0)
        self.img_w = img_w
        self.img_h = img_h

        # å¦‚æœé‚„æ²’è¨­å®šç›¸æ©ŸçŸ©é™£ï¼Œè¨­ä¸€å€‹ä¼°è¨ˆå€¼
        if self.cam_matrix is None:
            focal_length = img_w
            center = (img_w / 2, img_h / 2)
            self.cam_matrix = np.array(
                [[focal_length, 0, center[0]], [0, focal_length, center[1]], [0, 0, 1]],
                dtype="double",
            )

        # 1. å¾ MediaPipe æå–å°æ‡‰çš„ 6 å€‹ 2D é—œéµé»
        # æ³¨æ„ï¼šMediaPipe çš„é»æ˜¯æ­£è¦åŒ–çš„ (0~1)ï¼Œè¦ä¹˜ä¸Šå¯¬é«˜
        # Index: Nose=1, Chin=152, L_Eye=33, R_Eye=263, L_Mouth=61, R_Mouth=291
        points = [
            (face_landmarks[i].x * img_w, face_landmarks[i].y * img_h)
            for i in pose_landmark_index
        ]

        image_points = np.array(points, dtype="double")

        # 2. å‘¼å« SolvePnP
        success, rotation_vector, translation_vector = cv2.solvePnP(
            self.model_points,
            image_points,
            self.cam_matrix,
            self.dist_coeffs,
            flags=cv2.SOLVEPNP_SQPNP,
        )

        # 3. å°‡æ—‹è½‰å‘é‡è½‰æ›ç‚ºæ­æ‹‰è§’ (Euler Angles)
        # é€™éƒ¨åˆ†æ•¸å­¸æ¯”è¼ƒæ·±ï¼Œä¸»è¦æ˜¯æŠŠæ—‹è½‰çŸ©é™£è½‰æˆæˆ‘å€‘çœ‹å¾—æ‡‚çš„è§’åº¦
        rmat, jac = cv2.Rodrigues(rotation_vector)
        angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)

        # 4. æå– Pitch, Yaw, Roll
        # æ ¹æ“š OpenCV çš„åº§æ¨™ç³»å®šç¾©ï¼š
        # angles[0] = Pitch (æŠ¬é ­ä½é ­)
        # angles[1] = Yaw (å·¦å³è½‰)
        # angles[2] = Roll (æ­ªé ­)

        pitch = angles[0]  # è½‰æ›æ¯”ä¾‹å¾®èª¿ (è¦–éœ€æ±‚èª¿æ•´å¼·åº¦)
        yaw = angles[1]
        roll = angles[2]
        if self.first:
            self.first = False
            self.init_angle = yaw, pitch, roll
        iyaw, ipitch, iroll = self.init_angle
        pitch -= ipitch
        yaw -= iyaw
        roll -= iroll
        self.last_angle = yaw, pitch, roll

        debug = True
        if debug:
            debug_board = np.zeros((480, 640, 3), dtype=np.uint8)
            frontal_points = self.get_frontal_landmarks(
                rmat, face_landmarks, img_w, img_h
            )
            for point in frontal_points:
                cv2.circle(debug_board, point, 1, (0, 255, 0), -1)  # ç•«ç¶ è‰²å°é»
            # ç•«å‡ºé¼»å°– (ç´…è‰²å¤§é») ä½œç‚ºåƒè€ƒä¸­å¿ƒ
            if len(frontal_points) > 1:
                for i in pose_landmark_index:
                    cv2.circle(debug_board, frontal_points[i], 3, (0, 0, 255), -1)
            cv2.imshow("Debug: Frontalized View", debug_board)
            # é¡¯ç¤ºè¦–çª—
            cv2.waitKey(1)
        return yaw, pitch, roll

    def get_frontal_landmarks(self, rmat, face_landmarks, img_w, img_h):
        """
        è¼¸å…¥ï¼šç•¶å‰æ­ªæ–œçš„ landmarks å’Œæ—‹è½‰å‘é‡ rvec
        è¼¸å‡ºï¼šè¢«ã€Œè½‰æ­£ã€å¾Œçš„ 2D landmarks åº§æ¨™åˆ—è¡¨ (ç”¨æ–¼ç¹ªåœ–)
        """
        # 1. å–å¾—æ—‹è½‰çŸ©é™£ R
        # R = self.rt
        # R = self.rt

        # R = self.rt
        # 2. è¨ˆç®—é€†å‘æ—‹è½‰çŸ©é™£ (è½‰ç½®çŸ©é™£)
        # é€™å€‹çŸ©é™£çš„ä½œç”¨æ˜¯æŠŠæ­ªçš„é ­è½‰æ­£
        # R_inv = np.eye(3)

        # 3. æ”¶é›†ç•¶å‰æ‰€æœ‰ landmarks çš„ 3D åº§æ¨™
        # MediaPipe æä¾›çš„ z åº§æ¨™æ˜¯ç›¸å°æ·±åº¦ï¼Œæˆ‘å€‘éœ€è¦æŠŠå®ƒè®Šæˆé¡ä¼¼åƒç´ çš„å–®ä½
        landmarks_3d_list = []
        for lm in face_landmarks:
            # å°‡æ¨™æº–åŒ–åº§æ¨™è½‰æ›ç‚ºè¿‘ä¼¼çš„ 3D ç©ºé–“åº§æ¨™
            # x, y ä¹˜ä¸Šå¯¬é«˜ï¼Œz ä¹Ÿä¹˜ä¸Šå¯¬åº¦ä½œç‚ºæ·±åº¦æ¯”ä¾‹ä¼°è¨ˆ
            lx, ly, lz = lm.x * img_w, lm.y * img_h, lm.z * img_w
            landmarks_3d_list.append([lx, ly, lz])

        points_np = np.array(landmarks_3d_list, dtype=np.float32)
        unrotated_points_3d = points_np
        frontal_points_2d = []

        min_x = np.min(unrotated_points_3d[:, 0])
        max_x = np.max(unrotated_points_3d[:, 0])
        min_y = np.min(unrotated_points_3d[:, 1])
        max_y = np.max(unrotated_points_3d[:, 1])

        face_w = max_x - min_x
        face_h = max_y - min_y

        # B. è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹ (Scale)
        # æˆ‘å€‘å¸Œæœ›è‡‰çš„å¯¬åº¦ä½”ç•«é¢çš„ padding_ratio
        # æˆ–è€…æ˜¯é«˜åº¦ä½”ç•«é¢çš„ padding_ratio
        # å–å…©è€…ä¸­è¼ƒå°çš„å€¼ï¼Œç¢ºä¿ä¸æœƒè¶…å‡ºç•«é¢
        padding_ratio = 0.8
        scale_x = (img_w * padding_ratio) / face_w
        scale_y = (img_h * padding_ratio) / face_h
        final_scale = min(scale_x, scale_y)
        # C. è¨ˆç®—ä¸­å¿ƒåç§» (Centering)
        # æˆ‘å€‘è¦æŠŠè‡‰çš„ "å¹¾ä½•ä¸­å¿ƒ" ç§»åˆ° "è¦–çª—ä¸­å¿ƒ"
        face_center_x = (min_x + max_x) / 2
        face_center_y = (min_y + max_y) / 2

        window_center_x = img_w // 2
        window_center_y = img_h // 2

        frontal_points_2d = []
        for p3d in unrotated_points_3d:
            px = int((p3d[0] - face_center_x) * final_scale + window_center_x)
            py = int((p3d[1] - face_center_y) * final_scale + window_center_y)
            frontal_points_2d.append((px, py))

        return frontal_points_2d

    def release(self):
        self.cap.release()


class AsyncFaceTracker:
    """
    éåŒæ­¥è¿½è¹¤å™¨ï¼šç”¨ç¨ç«‹åŸ·è¡Œç·’è·‘ OpenCVï¼Œ
    ç¢ºä¿ä¸»è¦–çª—è¢« Windows å¡ä½æ™‚ (ä¾‹å¦‚æ‹–æ›³è¦–çª—)ï¼Œè¿½è¹¤ä¸æœƒä¸­æ–·ã€‚
    """

    def __init__(self):
        # å»ºç«‹åŸæœ¬çš„ Tracker
        self._tracker = FaceTracker()

        # å…±äº«è®Šæ•¸ (åŠ ä¸Š Lock é¿å…è®€å¯«è¡çªï¼Œé›–ç„¶ Python GIL æŸç¨®ç¨‹åº¦ä¸Šæœƒä¿è­·)
        self.lock = threading.Lock()
        self._current_iris_pos = (0.0, 0.0)
        self._current_blink_ratio = (1.0, 1.0)
        self._current_head_pose = (0.0, 0.0)

        self.running = True

        # å»ºç«‹ä¸¦å•Ÿå‹•åŸ·è¡Œç·’
        self.thread = threading.Thread(target=self._update_loop, daemon=True)
        self.thread.start()

    def _update_loop(self):
        """é€™æ˜¯èƒŒæ™¯åŸ·è¡Œç·’åœ¨åšçš„äº‹ï¼šä¸æ–·æ›´æ–°æ•¸æ“š"""
        while self.running:
            img = self._tracker.process()
            height, width, channels = img.shape
            # 1. å–å¾—æ•¸æ“š (é€™ä¸€æ­¥æœ€è€—æ™‚ï¼Œç¾åœ¨ä¸æœƒå¡ä½ UI äº†)
            if self._tracker.results is None:
                continue
            yaw, pitch, roll = self._tracker.get_head_pose(width, height)
            bl, br = self._tracker.get_eye_blink_ratio()
            dx, dy = self._tracker.get_iris_pos()
            mo = self._tracker.calculate_mouth_openness()

            pub.sendMessage(
                "FaceInfo",
                face_info={
                    "PupilsPos": (dx, dy),
                    "Blinking": (bl, br),
                    "MouthOpenness": mo,
                    "Pose": (yaw, pitch, roll),
                },
            )
            # ç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…åƒå…‰ CPU (ç´„ 60 FPS)
            time.sleep(0.016)

    def release(self):
        self.running = False
        if self.thread.is_alive():
            self.thread.join()
        self._tracker.release()


class FakeTracker:
    """
    éåŒæ­¥è¿½è¹¤å™¨ï¼šç”¨ç¨ç«‹åŸ·è¡Œç·’è·‘ OpenCVï¼Œ
    ç¢ºä¿ä¸»è¦–çª—è¢« Windows å¡ä½æ™‚ (ä¾‹å¦‚æ‹–æ›³è¦–çª—)ï¼Œè¿½è¹¤ä¸æœƒä¸­æ–·ã€‚
    """

    def __init__(self):
        pub.sendMessage(
            "FaceInfo",
            face_info={
                "PupilsPos": (0, 0),
                "Blinking": (0, 0),
                "MouthOpenness": 0,
                "Pose": (0, 0, 0),
            },
        )

        self.running = True
        self.thread = threading.Thread(target=self._update_loop, daemon=True)
        self.thread.start()

    def _update_loop(self):
        """é€™æ˜¯èƒŒæ™¯åŸ·è¡Œç·’åœ¨åšçš„äº‹ï¼šä¸æ–·æ›´æ–°æ•¸æ“š"""
        while self.running:
            pub.sendMessage(
                "FaceInfo",
                face_info={
                    "PupilsPos": (0, 0),
                    "Blinking": (1, 1),
                    "MouthOpenness": 0,
                    "Pose": (0, 0, 0),
                },
            )
            # ç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…åƒå…‰ CPU (ç´„ 60 FPS)
            time.sleep(0.016)

    def release(self):
        self.running = False
        return
